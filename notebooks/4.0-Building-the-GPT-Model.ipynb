{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f869dd-0c0e-4980-b956-f942b350be31",
   "metadata": {},
   "source": [
    "# Chapter 3: Building the Full GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb9139-8013-4cc8-a0b3-bfd03715e581",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous chapters, we built the core components of a transformer, most importantly the `MultiHeadAttention` module. Now, it's time to assemble these pieces into a complete, functional GPT model.\n",
    "\n",
    "Our approach will be \"top-down.\" First, we will define the high-level architecture of the entire model using simple placeholders for the complex parts. This will help us understand the overall data flow. \n",
    "Then, in the following sections, we will build the real components to replace these placeholders one by one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8727b-d39e-4888-bb95-dff410e2f6d6",
   "metadata": {},
   "source": [
    "## 3.1 Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75630c39-103a-482c-8aae-283179b0f12e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "As always, we begin by importing the necessary libraries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d54abba-c497-4f75-95b2-e4163d0cc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library and third-party imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Add Project Root to Python Path ---\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "current_notebook_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# --- Imports from your `src` package ---\n",
    "from src.attention import MultiHeadAttention\n",
    "# from gpt_from_scratch.layers import LayerNorm, FeedForward, GELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63285108-d071-4537-89d1-0acf32271fe2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2 Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a76ac-9a34-44da-9203-d2562f328e9f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "First, let's define a configuration dictionary that will hold all the hyperparameters for our model. We will base this on the parameters of the original GPT-2 \"small\" model, which has 124 million parameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45dde7b4-05bc-41ef-8f87-a6b66a145789",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"dropout_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias in attention\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788492b-e698-4f49-8042-68a8c18af695",
   "metadata": {},
   "source": [
    "## 3.3 The GPT Architecture I: A High Level View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9bc1c-850a-4f77-9f98-8735c94aee94",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A \"Top-Down\" Approach</b><br>\n",
    "    \n",
    "Before we assemble the final, complex components of our GPT model, it's helpful to first understand the big picture. In this section, we'll adopt a **\"top-down\"** design approach by building a high-level **\"skeleton\"** of the entire model.\n",
    "</div>\n",
    "    \n",
    "\n",
    "We will use simple dummy classes as placeholders for the internal machinery, like the `TransformerBlock` and `LayerNorm`. This strategy allows us to focus on the overall architecture and trace the journey of a tensor as it flows from the input embedding layer to the final output logits, ensuring our high-level structure is correct before we dive into implementing the details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4586306-0900-41dd-a94d-6ac298267a67",
   "metadata": {},
   "source": [
    "### 3.3.1 Defining the Model Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068def8-f249-4442-9fb2-d7a1df1467c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The code below defines our `DummyGPTModel`. It includes the complete end-to-end structure of the network. Notice the placeholder classes, `DummyTransformerBlock` and `DummyLayerNorm`, which are designed to simply pass the input through without any changes for now.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d298d3ff-9d67-4151-be12-d9b07e97dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "\n",
    "        # Use a placeholder for the stack of Transformer Blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Use a placeholder for the final LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits        \n",
    "\n",
    "\n",
    "# --- Placeholder Classes ---\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # This layer also does nothing and just returns its input\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf106db4-4660-44f5-a13a-75018d5f78b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The `DummyGPTModel` defines the complete, high-level architecture. Let's analyze the `forward` method to understand the data flow:\n",
    "\n",
    "1. **Embedding:** The input token IDs are converted into token embeddings. Simultaneously, positional embeddings are created for each token based on its position in the sequence. These two embeddings are added together to give each token semantic meaning and positional context.\n",
    "   \n",
    "2. **Transformer Blocks:** The resulting embeddings are processed through a stack of `n_layers` Transformer blocks. In this dummy version, these blocks don't do anything yet, but this is where the self-attention and feed-forward logic will go.\n",
    "\n",
    "3. **Final Normalization:** A final `LayerNorm` is applied after the Transformer Blocks.\n",
    "\n",
    "4. **Output Head:** A final linear layer projects the output of the transformers back into vocabulary space, producing raw, unnormalized scores (logits) for every possible token in the vocabulary.\n",
    "\n",
    "Now that we have this high-level skeleton, the next sections will focus on building the real, functional version of `LayerNorm` and `TransformerBlock` to replace these placeholders.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec3482-a5a7-44f7-9a16-701c1cae823b",
   "metadata": {},
   "source": [
    "### 3.3.2 Testing the GPT Model Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb405314-fcd0-4093-8da8-90fad432e564",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now that we have the high-level skeleton of our model, let's run a quick test to ensure the data flows through it correctly. We will perform two steps:\n",
    "    \n",
    "* 1.  First, we'll tokenize two sample sentences and create an input batch.\n",
    "* 2.  Second, we'll pass this batch through our `DummyGPTModel` and verify that the output tensor has the shape we expect.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af4dea40-854f-4a8f-8fbe-0053fc33d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\"Input shape:\", batch.shape)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f563706-c7fa-4e92-a3cb-411f3c99a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.1073, -0.3112,  0.4756,  ..., -0.0372,  1.4159,  0.0902],\n",
      "         [-0.8873,  0.4433,  0.1565,  ..., -0.2307,  0.2448, -0.8779],\n",
      "         [-2.3526, -0.9504, -0.7646,  ...,  0.7654,  0.2491,  0.3775],\n",
      "         [ 0.5979,  1.0420, -1.5600,  ...,  0.4073, -0.0435, -0.3111]],\n",
      "\n",
      "        [[-0.7754, -0.3975,  0.8207,  ..., -0.4550,  1.2873, -0.1645],\n",
      "         [-1.2486, -0.1166,  0.6246,  ...,  0.5602,  0.8671, -0.6983],\n",
      "         [-0.0773, -0.1393, -0.3316,  ..., -0.0703,  1.7652, -0.1684],\n",
      "         [-0.1209,  0.4775,  0.3865,  ...,  0.4667, -1.6924, -0.4930]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the DummyGPTModel\n",
    "torch.manual_seed(100)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# Pass the batch through the model\n",
    "logits = model(batch)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7becf95-d378-457b-8c81-6a0dba0a4e1a",
   "metadata": {},
   "source": [
    "### 3.3.3 Analyzing the Model's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c00391-e7f9-47f9-902e-4f65802fd0e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The output of our model is a tensor of **logits** with the shape `[2, 4, 50257]`. Let's breakdown what each dimension represents:\n",
    "\n",
    "* **`2` (Batch Size):** This corresponds to the two input sentences we provided (\"Every effort moves you\" and \"Every day holds a\").\n",
    "* **`4` (Sequence Length):** This corresponds to the four tokens in each input sentence.\n",
    "* **`50257` (Vocabulary Size):** For each of the 4 tokens in each sentence, the model has produced a vector of 50,257 raw, unnormalized scores. Each score corresponds to a unique token in the vocabulary.\n",
    "\n",
    "These logits represent the model's raw \"prediction\" for the **next** token in the sequence at each position. Later, we will pass these logits through a softmax function to turn them into probabilities and generate text.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Now that we have successfully taken a top-down look at the GPT architecture and verified its data flow, we will begin coding the real components, starting with the `LayerNorm` class to replace `DummyLayerNorm`.\n",
    "</div>    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fe547-7f80-4b4d-9bf8-1f846b617277",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.4 The GPT Architecture II: Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0eeeb9-4e1c-41e9-90d5-ed8310dfb781",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Layer Normalization is a critical technique used in transformers to stabilize the training process. Deep neural networks can suffer from a problem where the distribution of activations changes between layers, making training difficult. Layer Normalization helps by re-centering and re-scaling the activations at each layer to a standard distribution (with a mean of 0 and a variance of 1).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311946ab-3555-43c9-86cb-61c703e32387",
   "metadata": {},
   "source": [
    "### 3.4.1 Building LayerNorm from First Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cd570-16c1-4706-9822-e1c89e0bc87f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To understand what Layer Normalization does, let's start with a simple example. We'll create a mini-batch of random data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a79882-4f6e-495e-bf2e-6da65f470e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3607, -0.2859, -0.3938,  0.2429, -1.3833],\n",
      "        [-2.3134, -0.3172, -0.8660,  1.7482, -0.2759]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "# Create a batch of 2 examples with 5 features each\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0c449-c84e-466a-9e8e-fa40b5a151aa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As you can see, the output values are arbitrary. The purpose of LayerNorm is to rescale these activations.\n",
    "\n",
    "</div>\n",
    "\n",
    "Before we normalize, let's first calculate their current mean and variance for each example in the batch.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d61eaa-812a-41b8-ab74-5b166d990736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.2919],\n",
      "        [-0.4049]])\n",
      "Var:\n",
      " tensor([[0.4784],\n",
      "        [2.1288]])\n"
     ]
    }
   ],
   "source": [
    "mean = batch_example.mean(dim=-1, keepdim=True)\n",
    "var = batch_example.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Var:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8cab2-1db1-4d76-9b0b-111bb553b3de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now for the core operation, Layer Normalization standardizes the activations by subtracting the mean and dividing by the standard deviation (the square root of the variance).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eac87a9-431a-4696-b481-80d0e811bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized batch:\n",
      " tensor([[ 0.9435,  0.0086, -0.1474,  0.7733, -1.5780],\n",
      "        [-1.3081,  0.0601, -0.3161,  1.4757,  0.0884]])\n"
     ]
    }
   ],
   "source": [
    "# Manually apply normalization\n",
    "batch_norm = (batch_example - mean) / torch.sqrt(var)\n",
    "\n",
    "print(\"Normalized batch:\\n\", batch_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be5aea-529d-447c-9fde-5995da8850fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's verify the result. If the normalization worked, then the mean should be approximately 0 and the new variance should be 1. Note that the mean may not be exactly zero due to tiny floating-point inaccuracies.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93e98ccf-4fdb-4d83-9f5e-b8d542e42e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of normalized batch:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]])\n",
      "Var of normalized batch:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Verify the mean and variance of the normalized inputs\n",
    "mean_norm = batch_norm.mean(dim=-1, keepdim=True)\n",
    "var_norm = batch_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean of normalized batch:\\n\", mean_norm)\n",
    "print(\"Var of normalized batch:\\n\", var_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4bf92-a85e-4c1f-bb1e-55bb7db78005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba996ac8-a08e-4f1c-9b39-e79feb927d20",
   "metadata": {},
   "source": [
    "### 3.4.2 Encapsulating the Logic in a `LayerNorm` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc1faf-de71-4737-8eb7-8c17027e3a0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now that we understood the step-by-step math, we can encapsulate this logic into a reusable PyTorch module. A production-ready `LayerNorm` class also includes two extra trainable parameters (`scale` and `shift`) and a small `epsilon` term for numerical stability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9bd19df-5560-49a6-8bd9-c4be6d89c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49786f08-5f31-46cb-9ab8-3fe3c2f96b4e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>What do  `scale` and `shift` do?</b>\n",
    "\n",
    "While forcing the activation to have a mean of 0 and variance of 1 is great for stability, it can sometimes limit the expressive power of the network. `scale` (gamma) and `shift` (beta) are trainable parameters that allow the model to learn the optimal scale and shift for the normalized activations. In essence , it gives the network the ability to \"undo\" the normalization if it finds that a different distribution works better for that specific layer.\n",
    "\n",
    "<b>Note on `unbiased=False`</b>\n",
    "\n",
    "In statistics, the variance of a sample is often calculated by dividing by `n-1` (unbiased) instead of `n` (biased). We explicitly set `unbiased=False`to divide by `n`. For the large embedding dimensions used in LLMs, the difference is negligible. This choice ensures  our implementation is compatible with the original GPT-2 model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79509d96-fa92-4576-b898-74bf08d87220",
   "metadata": {},
   "source": [
    "### 3.4.3 Testing the `LayerNorm` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50b5e4-5837-42ba-98b3-c8578647b9cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Finally, let's instantiate our new `LayerNorm` class and test it on our original example data to confirm it works as expected.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa27bb4c-3ffc-4b77-aa18-231de9e72be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a3242-2d5b-4279-8d10-6ecdb408c507",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.5 The GPT Architecture III: The FeedForward Neural Network with GELU Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531e31d-7154-4613-81c1-df41f307db19",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Every transformer block contains two main sub-layers. The first is the multi-head attention mechanism we've already built. The second is a simple, position-wise **feed-forward neural network.**\n",
    "\n",
    "This network's job is to process each token's information independently after it has already gathered context from the attention layer. It adds computational depth and allows the model to learn more complex representations for each token.\n",
    "\n",
    "A key component of this network in GPT models is the **GELU activation function**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d2628-fc14-4827-a9a3-01d57cd0fd43",
   "metadata": {},
   "source": [
    "### 3.5.1 The GELU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb96506-da5a-4514-a3bd-e27e85f3af4a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "While `ReLU` is a common activation function, GPT models use a smoother alternative called **GELU (Gaussian Error Linear Unit)**. Let's implement the specific approximation of GELU used by GPT-2 and then visualize it to see how it compares to ReLU.\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = 0.5x \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715x^3\\right)\\right]\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $x$ is the input to the function.\n",
    "* $\\tanh$ is the hyperbolic tangent function.\n",
    "* $\\pi$ is the mathematical constant Pi.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b33aca8-3bb2-4740-a0dc-590befc9deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308cd3d1-1c31-4773-b1c0-7f5e1a1c84f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb8b95-50fb-461e-a856-a9312f013505",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "As we can see in the plot, GELU has several advantages over the standard ReLU function:\n",
    "\n",
    "* **It's smooth:** Unlike ReLU, which has a sharp corner at zero, GELU is a smooth function.This can lead to better, more stable optimization properties during model training.\n",
    "* **It's non-zero for negative values:** ReLU completely blocks any negative input by turning it to zero. GELU allows a small amount of negative information to pass through, which means neurons that receive negative input can still contribute to the learning process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d075d-312a-40c2-8750-c91c5c7762a8",
   "metadata": {},
   "source": [
    "### 3.5.2 Building the `FeedForward` Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b133ae2-35aa-42fb-a21c-5bbacb95e844",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now, let's use our `GELU ` activation to build the `FeedForward` module. A key feature of the transformer's feed-forward layer is that  it first **expands** the input embedding dimension (typically by a factor of 4) and then **contracts** it back down. This expand-and-contract design allows the model to learn richer, more complex representations for each token in a higher-dimensional space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9c34f3-a7cf-48f0-99dc-cf8f5a3707e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), #Expansion layer\n",
    "            GELU(), # Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), # Contraction layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b6241-62c3-48c3-a1ed-7685416e4630",
   "metadata": {},
   "source": [
    "### 3.5.3 Testing the `FeedForward` Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2b6d1d-37a1-4c82-b074-ba75be70a637",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's test out `FeedForward` module by passing a sample batch of data through it and verifying that the output shape is the same, as expected.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23d36138-10fb-4700-887f-b12271825a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Note: We can use the GPT_CONFIG_124M dictionary we defined earlier in the notebook\n",
    "\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 4, 768)  # A sample batch: (batch_size, num_tokens, emb_dim)\n",
    "out = ffn(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd82d4-e936-4764-9ab8-092f0836164a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "The uniformity in input and output dimensions (`[2, 4, 768]`) is a crucial design choice. It simplifies the overall GPT architecture by enabling multiple `TransformerBlock` layers to be stacked on top of each other seamlessly, without the need to adjust tensor dimensions between them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca540de-9ca6-4a33-93c0-f112316508ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.6 The GPT Architecture IV: Shortcut Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84f0ca-16ef-4cf2-9366-b15f54e68258",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "A critical innovation that enabled the training of very deep neural networks is the **shortcut** or **residual connection**. This technique helps to mitigate the \"vanishing gradient problem\", a common issue where the signal from the loss function becomes too weak to update the weights in the early layers of a deep network.\n",
    "\n",
    "In a transformer block, shortcut connections are used around both the self-attention and the feed-forward layers. Let's see why they are so important with a hands-on example.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31defa4f-0ce6-42ac-a52d-3a26c8b1d8ee",
   "metadata": {},
   "source": [
    "### 3.6.1 The Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0a3459-a120-434d-9a11-dffb6cc1faeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "To understand the problem, we'll first build a simple, deep neural network **without** shortcut connections. We will then perform a backward pass and observe the gradients diminish as they flow from the final layer back to the first.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "977cae6a-a60d-4e41-9e73-e1a2b154ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple deep network and a helper function to inspect gradients\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[i], layer_sizes[i+1]), GELU())\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            # Add shortcut connection if enable and shapes match\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    \"\"\"A helper function to compute and print the mean absolute gradient for each layer's weights.\"\"\"\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss (how far the output is from the target)\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c359f6-e70c-488f-a0d8-3df09ac1192e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, let's create an instance of this network with shortcuts disabled and calculate the gradients for each layer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b4f1705-d6e1-4804-b413-7f63ec49c69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00011775017628679052\n",
      "layers.1.0.weight has gradient mean of 0.0003875654365401715\n",
      "layers.2.0.weight has gradient mean of 0.0014669443480670452\n",
      "layers.3.0.weight has gradient mean of 0.001532275229692459\n",
      "layers.4.0.weight has gradient mean of 0.013530432246625423\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "\n",
    "print_gradients(model_without_shortcut, sample_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0732436-c8c9-4960-875d-d4626b6fc82b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Analysis: Vanishing Gradients</b><br>\n",
    "\n",
    "As we can see in the output above, the mean gradient value is much larger for the last layer (`layers.4.0.weight`) than it is for the first layer (`layers.1.0.weight`). The gradient signal shrinks or \"vanishes\" as it is propagated backward through the network. In a much deeper network, the gradients for the initial layers would become so close to zero that those layers would effectively stop learning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e8c5f-dd32-4f3d-bd03-b44e487dad23",
   "metadata": {},
   "source": [
    "### 3.6.2 The Solution: Adding Shortcut Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945aa466-9ef7-4e20-9a54-f06efe9d6729",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now, let's enable the shortcut connections. By simply setting `use_shortcut=True`, our `forward` method will add the input of each layer (`x`)  back to its output (`layer_output`). This creates a direct path for the gradient to flow backward through the network, bypassing the layers that can diminish it.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a67c415-cee9-4c33-92bc-5d0dc9f178b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.08600781857967377\n",
      "layers.1.0.weight has gradient mean of 0.44186362624168396\n",
      "layers.2.0.weight has gradient mean of 0.10087182372808456\n",
      "layers.3.0.weight has gradient mean of 0.250803679227829\n",
      "layers.4.0.weight has gradient mean of 1.407089114189148\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59aeb3d-e6ec-4dea-85be-d3526e3e7e7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Conclusion: Stable Gradients</b><br>\n",
    "\n",
    "Success! With the shortcut connection enabled, the gradient values in the early layers are much stronger and on the same order of magnitude as the later layers. The gradient is able to flow effectively through the entire network.\n",
    "\n",
    "This technique is a core building block of very large models like LLMs, as it facilitates more effective and stable training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae3d55-060c-4dfd-a294-abc9f0312d94",
   "metadata": {},
   "source": [
    "## 3.7 The GPT Architecture V: Assembling the Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be903084-ec46-4231-b939-5082f7557188",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We have now built all the necessary components: `MultiHeadAttention`, `LayerNorm`, `FeedForward`, and we understand the importance of `Shortcut Connections`. It is finally time to assemble them into a complete `TransformerBlock`.\n",
    "\n",
    "This block is the fundamental, repeating unit of the GPT model. The model's power comes from stacking many of these blocks on top of each other.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9cdea-9c86-4e2b-b87c-a10f00956579",
   "metadata": {},
   "source": [
    "### 3.7.1 Defining the `TransformerBlock` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc960257-7b9c-4531-9edc-61598d5a3b62",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The `TransformerBlock` class combines the two main sub-layers (*multi-head attention* and the *feed-forward network*). Crucially, it applies `LayerNorm` **before** each sub-layer and uses shortcut connections **after** each one. This \"pre-layer norm\" architecture is a modern design that leads to more stable training than the original transformer paper's \"post-layer norm\" approach.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c2d774-4f74-4c5f-a7e2-c22d33603696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First sub-layer: Multi-Head Attention\n",
    "        self.attn = MultiHeadAttention(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout_rate=cfg[\"dropout_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"])\n",
    "\n",
    "        # Second sub-layer: Feed-Forward Network\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        # Layer normalization for each sub-layer\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        # Dropout for the shortcut connection\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- Attention Block with Shortcut Connection ---\n",
    "        shortcut = x\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_output = self.attn(x_norm)\n",
    "        x = shortcut + self.drop_shortcut(attn_output)\n",
    "\n",
    "        # --- Feed-Forward Block with Shortcut Connection ---\n",
    "        shortcut = x\n",
    "        x_norm = self.norm2(x)\n",
    "        ff_output = self.ff(x_norm)\n",
    "        x = shortcut + self.drop_shortcut(ff_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7123a0c-9273-47c9-8395-419370cd758a",
   "metadata": {},
   "source": [
    "### 3.7.2 Testing the Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752dd6e-fa72-48c9-9c32-6abdb63fb20d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now, let's instantiate our `TransformerBlock` and pass a sample batch of data through it to verify that it maintains the input shape, as expected.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a0f99d9-5fe9-423e-a3bf-e86a06acd16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "# Create a sample input batch: (batch_size, num_tokens, emb_dim)\n",
    "x = torch.rand(2, 4, GPT_CONFIG_124M[\"emb_dim\"])\n",
    "\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9175ff-863a-4caf-a6c3-6498e1a2d999",
   "metadata": {},
   "source": [
    "## 3.8 Final Assembly: The Complete `GPTModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee4abcb-b691-4640-b9b2-faa8d48b2aeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "With the `TransformerBlock` now fully defined, we have the last and most important building block. We can now assemble all of our components -the *embeddings*, the *stack of transformer blocks*, and the final *output layers*- into the complete `GPTModel` architecture.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0ce33a-670a-4438-b692-59ceddb40779",
   "metadata": {},
   "source": [
    "### 3.8.1 Defining the `GPTModel` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "677e30e3-4a5c-4a80-9cac-f39a51e9ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "\n",
    "        # A sequential stack of Transformers\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cdf85-6f90-4c76-bf17-e8201623ceb0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The `GPTModel` class uses its `__init__` constructor to build the layers of the network based on the provided configuration (`cfg`):\n",
    "\n",
    "* **Embedding Layers:** It initializes `tok_emb` and `pos_emb` layers. These are responsible for comverting input token IDs into vectors and adding positional information. A `dropout` layer (`drop_emb`) is also initialized for regularization.\n",
    "\n",
    "* **Transformer Blocks:** It creates a sequential stack of the `TransformerBlock` modules we built previously. The number of blocks is determined by `n_layers` in the configuration dictionary.\n",
    "\n",
    "* **Final Layers:** A final `LayerNorm` is applied for stability, followed by a linear `out_head` layer. This output head projects the transformer's output back into the vocabulary space to generate a score for every possible token.\n",
    "\n",
    "\n",
    "The **forward** method then defines the data's journey through these layers, taking token IDs as input and producing the final `logits` (the raw scores for the next-token prediction.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95459da9-0b36-406d-959b-ae6bcfc85183",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We will convert these logits into tokens and text outputs in the next section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaeceaa-caea-4f6c-bbed-4197985ff3f7",
   "metadata": {},
   "source": [
    "### 3.8.2 Testing the Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd30d9-7f67-4570-9a7c-80643c4a5d2e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's now instantiate our complete `GPTModel` using the `GPT_CONFIG_124M` dictionary and feed it the sample batch we created earlier. This will verify that the data flows through the entire stack of components correctly and produces an output of the expected shape.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "404eb503-58e2-4708-9502-aa41bf31373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([2, 4])\n",
      "\n",
      "Output batch shape: torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model (batch)\n",
    "\n",
    "print(\"Input batch shape:\", batch.shape)\n",
    "print(\"\\nOutput batch shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f43b9-1604-4e2f-bcdf-dec8104620eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "As we can see, the output tensor has the shape `[2, 4, 50257]`, since we passed in 2 input texts with 4 tokens each. The final dimension, 50,257, correctly corresponds to the vocabulary size of the tokenizer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891b5b4-717e-43a7-8c4d-95ebc21c919b",
   "metadata": {},
   "source": [
    "### 3.8.3 Analyzing the Parameter Count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda5d155-88b4-4aea-a9d6-c7030691095d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now, let's see how large our model is by calculating its total number of parameters using the `.numel()` method, which is short for \"number of elements\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27982f4f-0d11-4dcc-bdc3-2849085a9b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252932b5-78af-4592-9727-a36e3aaf5cb3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>Why 163M Parameters Instead of 124M? A Note on Weight Tying</b><br>\n",
    "You might wonder why our model has 163 million parameters, while the GPT-2 model we're basing it on is famous for having 124 million. The reason is a concept called <b>weight tying</b>.\n",
    "\n",
    "The original GPT-2 architecture re-used the weights from the token embedding layer (`tok_emb`) as its final output layer (`out_head`). This saves a significant number of parameters. Our `GPTModel` uses separate weights for each, which is a more modern and often better-performing practice.\n",
    "\n",
    "If we subtract the parameters from our output head, we arrive at the original GPT-2 number.\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5662a1e-cf04-49f6-9181-512aebbf1587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n",
      "\n",
      "Number of parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "# Verify the shapes of the layers\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "\n",
    "# Calculate the parameter count without the final output layer\n",
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"\\nNumber of parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327ba45-b594-4aeb-aa83-8fd2fd8fd4c6",
   "metadata": {},
   "source": [
    "### 3.8.4 Calculating Memory Footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96e835-ace4-4747-a924-f110b32cccea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Finally, let's see how much memory these 163 million parameters wwill occupy. Assuming each parameter is a standard 32-bit float (which takes up 4 bytes), we can calculate the model's size.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "301d0c0d-1432-4e5d-8803-3d61fb8b197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Each parameter is a 32-bit float (4 bytes)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert bytes to megabytes (1 MB = 1024 * 1024 bytes)\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63fa60-a196-4521-92ef-26fa9d518385",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The total size of over 600 MB illustrates the significant storage and memory capacity required to accommodate even relatively small LLMs like this one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0376b-e74b-4893-87ca-84b7d9ef2be0",
   "metadata": {},
   "source": [
    "# Chapter 3 Summary and Next Steps\n",
    "\n",
    "This marks the end of our journey in building a complete GPT model architecture from scratch. We have successfully progressed from the most fundamental concepts of self-attention to assembling a full, functional language model, ready for training.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "  <b>Congratulations on This Major Accomplishment!</b><br>\n",
    "  Throughout these notebooks, we have:\n",
    "  <ul>\n",
    "    <li>Implemented the core <b>self-attention mechanism</b> from first principles.</li>\n",
    "    <li>Built essential layers like <b>LayerNorm</b>, <b>GELU</b>, and position-wise <b>Feed-Forward Networks</b>.</li>\n",
    "    <li>Scaled up our attention with a professional <b>Multi-Head Attention</b> module.</li>\n",
    "    <li>Assembled all these components into the fundamental, repeating <b>Transformer Block</b>.</li>\n",
    "    <li>Constructed the final <b>`GPTModel`</b>, capable of taking token IDs as input and producing logits as output.</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "### Where We Are Now\n",
    "We have a complete, architecturally correct GPT model with randomly initialized weights. As we saw in the text generation step, this untrained model produces incoherent gibberish. This is the expected and correct state at this stage. The \"brain\" has been built, but it has not yet learned anything.\n",
    "\n",
    "### What Are the Next Steps?\n",
    "Building the architecture is a huge part of the challenge, but the journey to a powerful LLM continues. The next logical steps would be:\n",
    "* **Training the Model:** Implementing a training loop that feeds the model large amounts of text data, calculates a loss (using cross-entropy), and updates the model's weights via an optimizer like AdamW.\n",
    "* **Scaling Up:** Training a model with more layers, more attention heads, and a larger embedding dimension on a massive dataset (many gigabytes of text), likely on GPU hardware.\n",
    "* **Improving Text Generation:** Implementing more sophisticated sampling techniques beyond simple `argmax` (greedy decoding), such as temperature sampling or nucleus sampling, to generate more creative and diverse text.\n",
    "* **Evaluation:** Establishing metrics like perplexity to objectively measure how well the model has learned the language after training.\n",
    "\n",
    "Thank you for following along on this deep dive. Understanding these models from the ground up provides a powerful foundation for working with and building upon any language model in the field."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
