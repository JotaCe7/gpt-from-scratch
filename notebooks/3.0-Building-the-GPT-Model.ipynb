{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f869dd-0c0e-4980-b956-f942b350be31",
   "metadata": {},
   "source": [
    "# Chapter 3: Building the Full GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb9139-8013-4cc8-a0b3-bfd03715e581",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous chapters, we built the core components of a transformer, most importantly the `MultiHeadAttention` module. Now, it's time to assemble these pieces into a complete, functional GPT model.\n",
    "\n",
    "Our approach will be \"top-down.\" First, we will define the high-level architecture of the entire model using simple placeholders for the complex parts. This will help us understand the overall data flow. \n",
    "Then, in the following sections, we will build the real components to replace these placeholders one by one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8727b-d39e-4888-bb95-dff410e2f6d6",
   "metadata": {},
   "source": [
    "## 3.1 Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75630c39-103a-482c-8aae-283179b0f12e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "As always, we begin by importing the necessary libraries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d54abba-c497-4f75-95b2-e4163d0cc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library and third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63285108-d071-4537-89d1-0acf32271fe2",
   "metadata": {},
   "source": [
    "## 3.2 Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a76ac-9a34-44da-9203-d2562f328e9f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "First, let's define a configuration dictionary that will hold all the hyperparameters for our model. We will base this on the parameters of the original GPT-2 \"small\" model, which has 124 million parameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45dde7b4-05bc-41ef-8f87-a6b66a145789",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"dropout_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias in attention\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788492b-e698-4f49-8042-68a8c18af695",
   "metadata": {},
   "source": [
    "## 3.3 The GPT Architecture I: A High Level View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9bc1c-850a-4f77-9f98-8735c94aee94",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A \"Top-Down\" Approach</b><br>\n",
    "    \n",
    "Before we assemble the final, complex components of our GPT model, it's helpful to first understand the big picture. In this section, we'll adopt a **\"top-down\"** design approach by building a high-level **\"skeleton\"** of the entire model.\n",
    "</div>\n",
    "    \n",
    "\n",
    "We will use simple dummy classes as placeholders for the internal machinery, like the `TransformerBlock` and `LayerNorm`. This strategy allows us to focus on the overall architecture and trace the journey of a tensor as it flows from the input embedding layer to the final output logits, ensuring our high-level structure is correct before we dive into implementing the details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4586306-0900-41dd-a94d-6ac298267a67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3.1 Defining the Model Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068def8-f249-4442-9fb2-d7a1df1467c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The code below defines our `DummyGPTModel`. It includes the complete end-to-end structure of the network. Notice the placeholder classes, `DummyTransformerBlock` and `DummyLayerNorm`, which are designed to simply pass the input through without any changes for now.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d298d3ff-9d67-4151-be12-d9b07e97dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "\n",
    "        # Use a placeholder for the stack of Transformer Blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Use a placeholder for the final LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits        \n",
    "\n",
    "\n",
    "# --- Placeholder Classes ---\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # This layer also does nothing and just returns its input\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf106db4-4660-44f5-a13a-75018d5f78b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The `DummyGPTModel` defines the complete, high-level architecture. Let's analyze the `forward` method to understand the data flow:\n",
    "\n",
    "1. **Embedding:** The input token IDs are converted into token embeddings. Simultaneously, positional embeddings are created for eachtoken based on its position in the sequence. These two embeddings are added together to give each token semantic meaning and positional context.\n",
    "   \n",
    "2. **Transformer Blocks:** The resulting embeddings are preocessed through a stack of `n_layers` Transformer blocks. In this dummy version, these blocks don't do anything yet, but this is where the self-attention and feed-forwardlogic will go.\n",
    "\n",
    "3. **Final Normalization:** A final `LayerNorm` is applied after the Transformer Blocks.\n",
    "\n",
    "4. **Output Head:** A final linear layer projects the output of the transformers back into vocabulary space, producing raw, unnormalized scores (logits) for every possible token in the vocabulary.\n",
    "\n",
    "Now that we have this hisgh-level skeleton, the next sections will focus on building the real, functional version of `LayerNorm` and `TransformerBlock` to replace these placeholders.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec3482-a5a7-44f7-9a16-701c1cae823b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3.2 Testing the GPT Model Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb405314-fcd0-4093-8da8-90fad432e564",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now that we have the high-level skeleton of our model, let's run a quick test to ensure the data flows through it correctly. We will perform two steps:\n",
    "    \n",
    "1.  First, we'll tokenize two sample sentences and create an input batch.\n",
    "2.  Second, we'll pass this batch through our `DummyGPTModel` and verify that the output tensor has the shape we expect.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af4dea40-854f-4a8f-8fbe-0053fc33d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\"Input shape:\", batch.shape)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f563706-c7fa-4e92-a3cb-411f3c99a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.1073, -0.3112,  0.4756,  ..., -0.0372,  1.4159,  0.0902],\n",
      "         [-0.8873,  0.4433,  0.1565,  ..., -0.2307,  0.2448, -0.8779],\n",
      "         [-2.3526, -0.9504, -0.7646,  ...,  0.7654,  0.2491,  0.3775],\n",
      "         [ 0.5979,  1.0420, -1.5600,  ...,  0.4073, -0.0435, -0.3111]],\n",
      "\n",
      "        [[-0.7754, -0.3975,  0.8207,  ..., -0.4550,  1.2873, -0.1645],\n",
      "         [-1.2486, -0.1166,  0.6246,  ...,  0.5602,  0.8671, -0.6983],\n",
      "         [-0.0773, -0.1393, -0.3316,  ..., -0.0703,  1.7652, -0.1684],\n",
      "         [-0.1209,  0.4775,  0.3865,  ...,  0.4667, -1.6924, -0.4930]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the DummyGPTModel\n",
    "torch.manual_seed(100)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# Pass the batch through the model\n",
    "logits = model(batch)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7becf95-d378-457b-8c81-6a0dba0a4e1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.3.3 Analyzing the Model's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c00391-e7f9-47f9-902e-4f65802fd0e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The output of our model is a tensor of **logits** with the shape `[2, 4, 50257]`. Let's breakdown what each dimension represents:\n",
    "\n",
    "* **`2` (Batch Size):** This corresponds to the two input sentences we provided (\"Every effort moves you\" and \"Every day holds a\").\n",
    "* **`4` (Sequence Length):** This corresponds to the four tokens in each input sentence.\n",
    "* **`50257` (Vocabulary Size):** For each of the 4 tokens in each sentence, the model has produced a vector of 50,257 raw, unnormalized scores. Each score corresponds to a unique token in the vocabulary.\n",
    "\n",
    "These logits represent the model's raw \"prediction\" for the **next** token in the sequence at each position. Later, we will pass these logits through a softmax function to turn them into probabilities and generate text.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Now that we have successfully taken a top-down look at the GPT architecture and verified its data flow, we will begin coding the real components, starting with the `LayerNorm` class to replace `DummyLayerNorm`.\n",
    "</div>    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fe547-7f80-4b4d-9bf8-1f846b617277",
   "metadata": {},
   "source": [
    "## 3.4 The GPT Architecture II: Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0eeeb9-4e1c-41e9-90d5-ed8310dfb781",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Layer Normalization is a critical technique used in transformers to stabilize the training process. Deep neural networks can suffer from a problem where the distribution of activations changes between layers, making training difficult. Layer Normalization helps by re-centering and re-scaling the activations at each layer to a standard distribution (with a mean of 0 and a variance of 1).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311946ab-3555-43c9-86cb-61c703e32387",
   "metadata": {},
   "source": [
    "### 3.4.1 Building LayerNorm from First Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90cd570-16c1-4706-9822-e1c89e0bc87f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To understand what Layer Normalization does, let's start with a simple example. We'll create a mini-batch of random data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6a79882-4f6e-495e-bf2e-6da65f470e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3607, -0.2859, -0.3938,  0.2429, -1.3833],\n",
      "        [-2.3134, -0.3172, -0.8660,  1.7482, -0.2759]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "# Create a batch of 2 examples with 5 features each\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "print(batch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0c449-c84e-466a-9e8e-fa40b5a151aa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As you can see, the output values are arbitrary. The purpose of LayerNorm is to rescale these activations.\n",
    "\n",
    "</div>\n",
    "\n",
    "Before we normalize, let's first calculate their current mean and variance for each example in the batch.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05d61eaa-812a-41b8-ab74-5b166d990736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.2919],\n",
      "        [-0.4049]])\n",
      "Var:\n",
      " tensor([[0.4784],\n",
      "        [2.1288]])\n"
     ]
    }
   ],
   "source": [
    "mean = batch_example.mean(dim=-1, keepdim=True)\n",
    "var = batch_example.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Var:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8cab2-1db1-4d76-9b0b-111bb553b3de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now for the core operation, Layer Normalization standardizes the activations by subtracting the mean and dividing by the standard deviation (the square root of the variance).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2eac87a9-431a-4696-b481-80d0e811bc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized batch:\n",
      " tensor([[ 0.9435,  0.0086, -0.1474,  0.7733, -1.5780],\n",
      "        [-1.3081,  0.0601, -0.3161,  1.4757,  0.0884]])\n"
     ]
    }
   ],
   "source": [
    "# Manually apply normalization\n",
    "batch_norm = (batch_example - mean) / torch.sqrt(var)\n",
    "\n",
    "print(\"Normalized batch:\\n\", batch_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be5aea-529d-447c-9fde-5995da8850fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's verify the result. If the normalization worked, then the mean should be approximately 0 and the new variance should be 1. Note that the mean may not be exactly zero due to tiny floating-point inaccuracies.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93e98ccf-4fdb-4d83-9f5e-b8d542e42e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of normalized batch:\n",
      " tensor([[     0.0000],\n",
      "        [    -0.0000]])\n",
      "Var of normalized batch:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Verify the mean and variance of the normalized inputs\n",
    "mean_norm = batch_norm.mean(dim=-1, keepdim=True)\n",
    "var_norm = batch_norm.var(dim=-1, keepdim=True)\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean of normalized batch:\\n\", mean_norm)\n",
    "print(\"Var of normalized batch:\\n\", var_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4bf92-a85e-4c1f-bb1e-55bb7db78005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba996ac8-a08e-4f1c-9b39-e79feb927d20",
   "metadata": {},
   "source": [
    "### 3.4.2 Encapsulating the Logic in a `LayerNorm` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc1faf-de71-4737-8eb7-8c17027e3a0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now that we understood the step-by-step math, we can encapsulate this logic into a reusable PyTorch module. A production-ready `LayerNorm` class also includes two extra trainable parameters (`scale` and `shift`) and a small `epsilon` term for numerical stability.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9bd19df-5560-49a6-8bd9-c4be6d89c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49786f08-5f31-46cb-9ab8-3fe3c2f96b4e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<b>What do  `scale` and `shift` do?</b>\n",
    "\n",
    "While forcing the activation to have a mean of 0 and variance of 1 is great for stability, it can sometimes limit the expressive power of the network. `scale` (gamma) and `shift` (beta) are trainable parameters that allow the model to learn the optimal scale and shift for the normalized activations. In essence , it gives the network the ability to \"undo\" the normalization if it finds that a different distribution works better for that specific layer.\n",
    "\n",
    "<b>Note on `unbiased=False`</b>\n",
    "\n",
    "In statistics, the variance of a sample is often calculated by dividing by `n-1` (unbiased) instead of `n` (biased). We explicitly set `unbiased=False`to divide by `n`. For the large embedding dimensions used in LLMs, the difference is negligible. This choice ensures  our implementation is compatible with the original GPT-2 model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79509d96-fa92-4576-b898-74bf08d87220",
   "metadata": {},
   "source": [
    "### 3.4.3 Testing the `LayerNorm` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50b5e4-5837-42ba-98b3-c8578647b9cf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Finally, let's instantiate our new `LayerNorm` class and test it on our original example data to confirm it works as expected.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa27bb4c-3ffc-4b77-aa18-231de9e72be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [    -0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
