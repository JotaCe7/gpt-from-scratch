{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f869dd-0c0e-4980-b956-f942b350be31",
   "metadata": {},
   "source": [
    "# Chapter 3: Building the Full GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bb9139-8013-4cc8-a0b3-bfd03715e581",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous chapters, we built the core components of a transformer, most importantly the `MultiHeadAttention` module. Now, it's time to assemble these pieces into a complete, functional GPT model.\n",
    "\n",
    "Our approach will be \"top-down.\" First, we will define the high-level architecture of the entire model using simple placeholders for the complex parts. This will help us understand the overall data flow. \n",
    "Then, in the following sections, we will build the real components to replace these placeholders one by one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e8727b-d39e-4888-bb95-dff410e2f6d6",
   "metadata": {},
   "source": [
    "## 3.1 Imports and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75630c39-103a-482c-8aae-283179b0f12e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "As always, we begin by importing the necessary libraries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d54abba-c497-4f75-95b2-e4163d0cc3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library and third-party imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63285108-d071-4537-89d1-0acf32271fe2",
   "metadata": {},
   "source": [
    "## 3.2 Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a76ac-9a34-44da-9203-d2562f328e9f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "First, let's define a configuration dictionary that will hold all the hyperparameters for our model. We will base this on the parameters of the original GPT-2 \"small\" model, which has 124 million parameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45dde7b4-05bc-41ef-8f87-a6b66a145789",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"dropout_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias in attention\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788492b-e698-4f49-8042-68a8c18af695",
   "metadata": {},
   "source": [
    "## 3.3 The GPT Architecture I: A High Level View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f9bc1c-850a-4f77-9f98-8735c94aee94",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A \"Top-Down\" Approach</b><br>\n",
    "    \n",
    "Before we assemble the final, complex components of our GPT model, it's helpful to first understand the big picture. In this section, we'll adopt a **\"top-down\"** design approach by building a high-level **\"skeleton\"** of the entire model.\n",
    "</div>\n",
    "    \n",
    "\n",
    "We will use simple dummy classes as placeholders for the internal machinery, like the `TransformerBlock` and `LayerNorm`. This strategy allows us to focus on the overall architecture and trace the journey of a tensor as it flows from the input embedding layer to the final output logits, ensuring our high-level structure is correct before we dive into implementing the details.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4586306-0900-41dd-a94d-6ac298267a67",
   "metadata": {},
   "source": [
    "### 3.3.1 Defining the Model Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068def8-f249-4442-9fb2-d7a1df1467c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The code below defines our `DummyGPTModel`. It includes the complete end-to-end structure of the network. Notice the placeholder classes, `DummyTransformerBlock` and `DummyLayerNorm`, which are designed to simply pass the input through without any changes for now.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d298d3ff-9d67-4151-be12-d9b07e97dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"dropout_rate\"])\n",
    "\n",
    "        # Use a placeholder for the stack of Transformer Blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # Use a placeholder for the final LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits        \n",
    "\n",
    "\n",
    "# --- Placeholder Classes ---\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # This layer also does nothing and just returns its input\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf106db4-4660-44f5-a13a-75018d5f78b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The `DummyGPTModel` defines the complete, high-level architecture. Let's analyze the `forward` method to understand the data flow:\n",
    "\n",
    "1. **Embedding:** The input token IDs are converted into token embeddings. Simultaneously, positional embeddings are created for eachtoken based on its position in the sequence. These two embeddings are added together to give each token semantic meaning and positional context.\n",
    "   \n",
    "2. **Transformer Blocks:** The resulting embeddings are preocessed through a stack of `n_layers` Transformer blocks. In this dummy version, these blocks don't do anything yet, but this is where the self-attention and feed-forwardlogic will go.\n",
    "\n",
    "3. **Final Normalization:** A final `LayerNorm` is applied after the Transformer Blocks.\n",
    "\n",
    "4. **Output Head:** A final linear layer projects the output of the transformers back into vocabulary space, producing raw, unnormalized scores (logits) for every possible token in the vocabulary.\n",
    "\n",
    "Now that we have this hisgh-level skeleton, the next sections will focus on building the real, functional version of `LayerNorm` and `TransformerBlock` to replace these placeholders.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec3482-a5a7-44f7-9a16-701c1cae823b",
   "metadata": {},
   "source": [
    "### 3.3.2 Testing the GPT Model Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb405314-fcd0-4093-8da8-90fad432e564",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now that we have the high-level skeleton of our model, let's run a quick test to ensure the data flows through it correctly. We will perform two steps:\n",
    "    \n",
    "1.  First, we'll tokenize two sample sentences and create an input batch.\n",
    "2.  Second, we'll pass this batch through our `DummyGPTModel` and verify that the output tensor has the shape we expect.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af4dea40-854f-4a8f-8fbe-0053fc33d35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\"Input shape:\", batch.shape)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0f563706-c7fa-4e92-a3cb-411f3c99a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.1073, -0.3112,  0.4756,  ..., -0.0372,  1.4159,  0.0902],\n",
      "         [-0.8873,  0.4433,  0.1565,  ..., -0.2307,  0.2448, -0.8779],\n",
      "         [-2.3526, -0.9504, -0.7646,  ...,  0.7654,  0.2491,  0.3775],\n",
      "         [ 0.5979,  1.0420, -1.5600,  ...,  0.4073, -0.0435, -0.3111]],\n",
      "\n",
      "        [[-0.7754, -0.3975,  0.8207,  ..., -0.4550,  1.2873, -0.1645],\n",
      "         [-1.2486, -0.1166,  0.6246,  ...,  0.5602,  0.8671, -0.6983],\n",
      "         [-0.0773, -0.1393, -0.3316,  ..., -0.0703,  1.7652, -0.1684],\n",
      "         [-0.1209,  0.4775,  0.3865,  ...,  0.4667, -1.6924, -0.4930]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the DummyGPTModel\n",
    "torch.manual_seed(100)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "# Pass the batch through the model\n",
    "logits = model(batch)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7becf95-d378-457b-8c81-6a0dba0a4e1a",
   "metadata": {},
   "source": [
    "### 3.3.3 Analyzing the Model's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c00391-e7f9-47f9-902e-4f65802fd0e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The output of our model is a tensor of **logits** with the shape `[2, 4, 50257]`. Let's breakdown what each dimension represents:\n",
    "\n",
    "* **`2` (Batch Size):** This corresponds to the two input sentences we provided (\"Every effort moves you\" and \"Every day holds a\").\n",
    "* **`4` (Sequence Length):** This corresponds to the four tokens in each input sentence.\n",
    "* **`50257` (Vocabulary Size):** For each of the 4 tokens in each sentence, the model has produced a vector of 50,257 raw, unnormalized scores. Each score corresponds to a unique token in the vocabulary.\n",
    "\n",
    "These logits represent the model's raw \"prediction\" for the **next** token in the sequence at each position. Later, we will pass these logits through a softmax function to turn them into probabilities and generate text.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Now that we have successfully taken a top-down look at the GPT architecture and verified its data flow, we will begin coding the real components, starting with the `LayerNorm` class to replace `DummyLayerNorm`.\n",
    "</div>    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fe547-7f80-4b4d-9bf8-1f846b617277",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af8e13-cbdc-44a2-80fe-c451770081f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7570b417-b650-4877-a8ac-18e92b72bfe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554aabfa-9bb7-4625-8128-d56261cb87dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
