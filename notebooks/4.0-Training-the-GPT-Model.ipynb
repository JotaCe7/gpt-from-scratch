{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff64fec-d916-4581-9720-9b2dd3d8963b",
   "metadata": {},
   "source": [
    "# Chapter 4: Training a GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80d7fe-ca02-40c7-b3ce-857edc6fabeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In the previous chapters, we successfully designed and built every architectural component of a GPT model from scratch. We now have a complete, functional `GPTModel` class ready to be used.\n",
    "\n",
    "However, the model currently has randomly initialized weights and knows nothing about language. In this chapter, we will take the crucial next step: **training the model**. We will feed it a text corpus, calculate its performance using a loss function, and iteratively update its weights to teach it to generate coherent text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c245b-1326-4703-9607-69aa347a2a81",
   "metadata": {},
   "source": [
    "## 4.1 Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabaddef-e0c9-42ab-9c3d-22641903c99c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We'll begin by importing the necessary libraries and the `GPTModel` we finalized in the last chapter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3403fa84-e80f-492a-aeaf-250bac91bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library and third-party imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "# --- Add Project Root to Python Path ---\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "current_notebook_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "\n",
    "# --- Imports from your `src` package ---\n",
    "from src.model import GPTModel\n",
    "\n",
    "# We will use the same configuration dictionary\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428a27c-07f5-46e8-a95f-40f0d5ae4c0c",
   "metadata": {},
   "source": [
    "## 4.2 Generating Text with the Untrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da85e3f-1cc9-4055-a18a-cf7bb5df3e53",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Before we train the model, let's see what it produces with its random, untrained weights. To do this, we need a function that can perform an **autoregressive** generation loop. This process involves feeding the model an initial context, predicting the next token, adding that token back to the context, and repeating the process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7fa2bf-a2d5-4155-aad3-f8276162c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context if exceeds the supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions from the model\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "\n",
    "         # Focus only on the prediction for the very last token in the sequence\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the token ID with the highest probability (greedy decoding)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append the new token ID to therunning sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86a846-962e-4ed8-a2e2-9e6a08200c04",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The `generate_text_simple` function implements **greedy decoding**. In this autoregressive process, the model repeatedly predicts the single most likely next token, appends it to the sequence, and feeds the new sequence back into the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd40bb-5d4b-4b8e-a937-aacbd8b20bd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A Note on `softmax` and `argmax`</b><br>\n",
    "  \n",
    "  In our function, we include a `softmax` step to convert the model's output scores (logits) into probabilities before finding the most likely token with `argmax`.\n",
    "\n",
    "  However, since `softmax` doesn't change the order of the scores, applying `argmax` directly to the `logits` would produce the exact same result. We include the step here to clearly illustrate the full process of generating probabilities, but it is technically redundant for greedy decoding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd5e55-d4da-4160-ae00-63f05562717b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Later in this chapter, when we will implement the GPT training code, we will also introduce additional sampling techniques where we modify the softmax outputs such that the model doesn't always select the most likely token, which introduces variability and creativity in the generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e2fee-d34e-4a40-a60a-f10d94c42572",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, let's test our `generate_text_simple` function. We'll provide it with the starting context \"Hello, I am\" by first encoding the string into a batch of token IDs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31d128b-b3a3-4731-8074-d115fa71a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "Encoded tensor shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input\n",
    "start_context = \"Hello, I am\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded:\", encoded)\n",
    "print(\"Encoded tensor shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e18ab9-0c50-4c3c-a388-e60eb24c5522",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we'll set the model to evaluation mode with `model.eval()`. This disables random components like dropout that are only used during training. We can then call our function to generate new tokens from the starting context.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380b5437-b208-4621-8403-b506f4069d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiate the Model ---\n",
    "torch.manual_seed(100)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model instantiated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c3fa0-7596-47d7-bc4e-96f499c65a55",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's generate text using our `generate_text_simple` function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a686c8-7fe8-4b6a-8596-520c5e5b1b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,  1908, 41574, 14356, 11426, 42884, 32296]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "output_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", output_ids)\n",
    "print(\"Output length:\", len(output_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647398c-cc88-4f9b-92fc-23e7adb54279",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Finally, we use the tokenizer's `.decode()` method to convert the output token IDs back into a readable string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333ee864-b9a4-48bc-a480-ca97cd8c7e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded text: 'Hello, I am sent agitated AW telephone Tomas shroud'\n"
     ]
    }
   ],
   "source": [
    "# Decode the output\n",
    "decoded_text = tokenizer.decode(output_ids.squeeze(0).tolist())\n",
    "print(f\"\\nDecoded text: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd523c-a439-404a-bf55-544edd3509c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "  <b>Why is the output gibberish?</b><br>\n",
    "  \n",
    "  As we can see, the generated text is incoherent. This is the correct and expected result at this stage.\n",
    "\n",
    "  The reason is that our model is completely **untrained**. Its weights are still the random values they were initialized with. It has not yet learned any patterns of the English language. This demonstration perfectly illustrates *why* we need to train the model. In the next sections, we will prepare a dataset and implement a training loop to do just that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e33c0-e19a-4358-92e8-a1252db2d4d2",
   "metadata": {},
   "source": [
    "## 4.3 Evaluating Generative Text Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed81d31-a18f-4bd2-808a-4c33e239a775",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous section, we saw that our untrained model produces incoherent gibberish. While we can see this qualitatively, we need a quantitative way to measure the model's performance. How do we capture \"good text\" versus \"bad text\" in a number that we can track and optimize?\n",
    "\n",
    "The answer is to use a **loss function**. For language models that predict next-token probabilities, the standard metric is **cross-entropy loss**. It measures how \"surprised\" the model is by the true next token; a lower loss means the model's predictions are closer to reality. A related, more interpretable metric is **perplexity**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde5b01-788f-4fdd-92ad-08005a691694",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we can calculate these metrics, we need to prepare our workspace. This involves two main setup steps:\n",
    "\n",
    "1.  **Initialize the Model:** We will instantiate a `GPTModel` using our configuration with a `context_length` of 256. Using a smaller context size (compared to the original GPT-2's 1024) reduces the computational requirements, making the examples in this chapter accessible on a standard laptop.\n",
    "\n",
    "2.  **Define Helper Functions:** We will define two convenience functions, `text_to_token_ids` and `token_ids_to_text`. These utilities will make it easier to convert back and forth between text and the model's numerical token IDs throughout our analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72d621b-f5a7-4adf-ae35-25e8619db674",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"dropout_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4c1d1cf-1d40-47a3-a47f-dc843c16c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0841ce7-4827-435d-a0a4-a15ed26a7826",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "With our model initialized and helper functions defined, we can now perform our first end-to-end text generation. We will provide the model with the starting context \"Every effort moves you\" and use the `generate_text_simple` function to have it autoregressively generate the next 10 tokens.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cef2123-3868-4565-916f-12b86cb977d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves yosu disclosing func intrusiveH bear confidently 480osit caric Psychic\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves yosu\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_contexta2, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb3081-b469-4a0d-bfc9-2949d80c177a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see above, the model does not produce good textbecause it has not been trained yet.\n",
    "\n",
    "**How do we measue or capture what \"good text\" is, in a numeric form, to track it during training?**\n",
    "\n",
    "The next subsection introduce metrics to calculate a loss metric for the generated output that we can use to measure the training progress.\n",
    "\n",
    "The next chapters on finetuning LLMs will also introduce additonal ways to neasure model quality.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
