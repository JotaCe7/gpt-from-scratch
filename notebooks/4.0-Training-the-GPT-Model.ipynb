{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff64fec-d916-4581-9720-9b2dd3d8963b",
   "metadata": {},
   "source": [
    "# Chapter 4: Training a GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80d7fe-ca02-40c7-b3ce-857edc6fabeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In the previous chapters, we successfully designed and built every architectural component of a GPT model from scratch. We now have a complete, functional `GPTModel` class ready to be used.\n",
    "\n",
    "However, the model currently has randomly initialized weights and knows nothing about language. In this chapter, we will take the crucial next step: **training the model**. We will feed it a text corpus, calculate its performance using a loss function, and iteratively update its weights to teach it to generate coherent text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c245b-1326-4703-9607-69aa347a2a81",
   "metadata": {},
   "source": [
    "## 4.1 Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabaddef-e0c9-42ab-9c3d-22641903c99c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We'll begin by importing the necessary libraries and the `GPTModel` we finalized in the last chapter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3403fa84-e80f-492a-aeaf-250bac91bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library and third-party imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "# --- Add Project Root to Python Path ---\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "current_notebook_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# --- Imports from your `src` package ---\n",
    "from src.model import GPTModel\n",
    "\n",
    "# We will use the same configuration dictionary\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428a27c-07f5-46e8-a95f-40f0d5ae4c0c",
   "metadata": {},
   "source": [
    "## 4.2 Generating Text with the Untrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da85e3f-1cc9-4055-a18a-cf7bb5df3e53",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Before we train the model, let's see what it produces with its random, untrained weights. To do this, we need a function that can perform an **autoregressive** generation loop. This process involves feeding the model an initial context, predicting the next token, adding that token back to the context, and repeating the process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7fa2bf-a2d5-4155-aad3-f8276162c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context if exceeds the supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions from the model\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "\n",
    "         # Focus only on the prediction for the very last token in the sequence\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the token ID with the highest probability (greedy decoding)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append the new token ID to therunning sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86a846-962e-4ed8-a2e2-9e6a08200c04",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The `generate_text_simple` function implements **greedy decoding**. In this autoregressive process, the model repeatedly predicts the single most likely next token, appends it to the sequence, and feeds the new sequence back into the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd40bb-5d4b-4b8e-a937-aacbd8b20bd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A Note on `softmax` and `argmax`</b><br>\n",
    "  \n",
    "  In our function, we include a `softmax` step to convert the model's output scores (logits) into probabilities before finding the most likely token with `argmax`.\n",
    "\n",
    "  However, since `softmax` doesn't change the order of the scores, applying `argmax` directly to the `logits` would produce the exact same result. We include the step here to clearly illustrate the full process of generating probabilities, but it is technically redundant for greedy decoding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd5e55-d4da-4160-ae00-63f05562717b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Later in this chapter, when we will implement the GPT training code, we will also introduce additional sampling techniques where we modify the softmax outputs such that the model doesn't always select the most likely token, which introduces variability and creativity in the generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e2fee-d34e-4a40-a60a-f10d94c42572",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, let's test our `generate_text_simple` function. We'll provide it with the starting context \"Hello, I am\" by first encoding the string into a batch of token IDs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31d128b-b3a3-4731-8074-d115fa71a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "Encoded tensor shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input\n",
    "start_context = \"Hello, I am\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded:\", encoded)\n",
    "print(\"Encoded tensor shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e18ab9-0c50-4c3c-a388-e60eb24c5522",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we'll set the model to evaluation mode with `model.eval()`. This disables random components like dropout that are only used during training. We can then call our function to generate new tokens from the starting context.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380b5437-b208-4621-8403-b506f4069d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiate the Model ---\n",
    "torch.manual_seed(100)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model instantiated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c3fa0-7596-47d7-bc4e-96f499c65a55",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's generate text using our `generate_text_simple` function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a686c8-7fe8-4b6a-8596-520c5e5b1b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,  1908, 41574, 14356, 11426, 42884, 32296]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "output_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", output_ids)\n",
    "print(\"Output length:\", len(output_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647398c-cc88-4f9b-92fc-23e7adb54279",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Finally, we use the tokenizer's `.decode()` method to convert the output token IDs back into a readable string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333ee864-b9a4-48bc-a480-ca97cd8c7e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded text: 'Hello, I am sent agitated AW telephone Tomas shroud'\n"
     ]
    }
   ],
   "source": [
    "# Decode the output\n",
    "decoded_text = tokenizer.decode(output_ids.squeeze(0).tolist())\n",
    "print(f\"\\nDecoded text: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd523c-a439-404a-bf55-544edd3509c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "  <b>Why is the output gibberish?</b><br>\n",
    "  \n",
    "  As we can see, the generated text is incoherent. This is the correct and expected result at this stage.\n",
    "\n",
    "  The reason is that our model is completely **untrained**. Its weights are still the random values they were initialized with. It has not yet learned any patterns of the English language. This demonstration perfectly illustrates *why* we need to train the model. In the next sections, we will prepare a dataset and implement a training loop to do just that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e33c0-e19a-4358-92e8-a1252db2d4d2",
   "metadata": {},
   "source": [
    "## 4.3 Evaluating Generative Text Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed81d31-a18f-4bd2-808a-4c33e239a775",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous section, we saw that our untrained model produces incoherent gibberish. While we can see this qualitatively, we need a quantitative way to measure the model's performance. How do we capture \"good text\" versus \"bad text\" in a number that we can track and optimize?\n",
    "\n",
    "The answer is to use a **loss function**. For language models that predict next-token probabilities, the standard metric is **cross-entropy loss**. It measures how \"surprised\" the model is by the true next token; a lower loss means the model's predictions are closer to reality. A related, more interpretable metric is **perplexity**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde5b01-788f-4fdd-92ad-08005a691694",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we can calculate these metrics, we need to prepare our workspace. This involves two main setup steps:\n",
    "\n",
    "1.  **Initialize the Model:** We will instantiate a `GPTModel` using our configuration with a `context_length` of 256. Using a smaller context size (compared to the original GPT-2's 1024) reduces the computational requirements, making the examples in this chapter accessible on a standard laptop.\n",
    "\n",
    "2.  **Define Helper Functions:** We will define two convenience functions, `text_to_token_ids` and `token_ids_to_text`. These utilities will make it easier to convert back and forth between text and the model's numerical token IDs throughout our analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72d621b-f5a7-4adf-ae35-25e8619db674",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"dropout_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(100)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c1d1cf-1d40-47a3-a47f-dc843c16c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0841ce7-4827-435d-a0a4-a15ed26a7826",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "With our model initialized and helper functions defined, we can now perform our first end-to-end text generation. We will provide the model with the starting context \"Every effort moves you\" and use the `generate_text_simple` function to have it autoregressively generate the next 10 tokens.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb6848-7f89-48d1-809f-73fc07acf9dd",
   "metadata": {},
   "source": [
    "start_context = \"Every effort moves yosu\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_contexta2, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb3081-b469-4a0d-bfc9-2949d80c177a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see above, the model does not produce good text because it has not been trained yet.\n",
    "\n",
    "**How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?**\n",
    "\n",
    "The next subsection introduces metrics to calculate a loss metric for the generated output that we can use to measure the training progress.\n",
    "\n",
    "The next chapters on finetuning LLMs will also introduce additional ways to neasure model quality.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d62330-d204-48ba-ac3c-bbeee69a693e",
   "metadata": {},
   "source": [
    "## 4.4 Calculating the Text Generation Loss: Cross-Entropy and Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76611d-820f-4ec6-b456-782e5a45be80",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Suppose we have an inputs tensor containing the token IDs for 2 training example (rows).\n",
    "\n",
    "Corresponding to the inputs, the targets contain the desired token IDs that we want the model to generate.\n",
    "\n",
    "Notice that the targets are the inputs shifted by 1 position, as we explained in *Chapter 1* when we implemented the data loader.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363a6635-9bdb-46c6-b3f1-5e02eba64b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b2634-1209-4bae-ad52-ca4529789d46",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each.\n",
    "\n",
    "Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary.\n",
    "\n",
    "Applying the softmax function, we can turn the logits into a tensor of the same dimension containing probability scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709f5674-22d4-4b26-a02c-415d6e46e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0ad7-bb08-4097-866c-81678f60c830",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As discussed in the previous section, we can apply the argmax function to convert theprobability scores into predicted token IDs.\n",
    "\n",
    "The softmax function above produced a 50,257-dimensional vector for each token; the argmax function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token.\n",
    "\n",
    "Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303b4a4b-1505-4bfe-85d4-f19537a6fbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[24926],\n",
      "         [49334],\n",
      "         [45732]],\n",
      "\n",
      "        [[ 3401],\n",
      "         [13257],\n",
      "         [46567]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54694150-4e7a-4991-a0a4-4501e4a8ea21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens.\n",
    "\n",
    "That's because the model wasn't trained yet.\n",
    "\n",
    "To train the model, we need to know how far it is away from the correct predictions (targets).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4951573b-bf87-4064-b78f-b5ae0aca7113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  huhbrainer Harding\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37f667-3331-4330-8052-56ac177e84d1",
   "metadata": {},
   "source": [
    "### 4.4.1 Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a9212-5474-4ee5-ba44-a031cacf7d11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The token probabilities corresponding to the target indices are as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2132b105-c4b5-435a-85eb-0134fc9ed32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([9.0643e-06, 6.8038e-06, 6.5535e-05])\n",
      "Text 2: tensor([1.0236e-04, 2.3294e-05, 1.6344e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6d381-7b7a-4b11-b2e3-5cb5acf6eba3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We want to maximize all these values, bringing them close to a probability of 1.\n",
    "\n",
    "In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e65405f-5023-419a-ba7e-efd56d94f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.6112, -11.8980,  -9.6329,  -9.1870, -10.6673, -11.0216])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc60899-fe26-4e19-a4fd-55dd9093710c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we compute the average log probability:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb12e499-67a9-4c86-a984-8984246272c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.6697)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probabililty for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b320a-0555-4d07-86b1-149129c2b13f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The goal is to make this average log probability as large as possible by optimizing the model weights.\n",
    "\n",
    "Due to the log, the largest possible value is 0, and we are currently far away from 0.\n",
    "\n",
    "In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.6697 so that it approaches 0, we would maximize 10.6697 so that it approaches 0.\n",
    "\n",
    "The negative value of -10.6697, i.e., 10.6697, is also called **cross-entropy loss** in deep learning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0ba7972-4829-4b69-ac5f-79e6b4186e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6697)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38045d8e-9d09-4c27-afb1-5411c1380d07",
   "metadata": {},
   "source": [
    "### 4.4.2 Cross-Entropy Loss Using PyTorch's Optimized Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6dd5c7-2cf3-4f01-97ee-f26c1e904929",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Thankfully, we don't have to do all that math manually. PyTorch provides a highly optimized `cross_entropy` function that performs the softmax and log-probability calculations for us.\n",
    "\n",
    "Before we can use it, we need to reshape our `logits` and `targets` tensors. The function expects the `logits` to have the shape `(N, C)` where `C` is the number of classes (our `vocab_size`), so we will flatten the batch and token dimensions together.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30163929-a78a-48a4-8ee1-30d759241d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60c38b-dc74-4a13-920b-1319d48f343c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For the **cross_entropy** function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98ca5f8a-d285-4421-8c37-8014732ecdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits shape: torch.Size([6, 50257])\n",
      "Flattened targets shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits shape:\", logits_flat.shape)\n",
    "print(\"Flattened targets shape:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390aaab1-62e0-432e-9be0-a01cd46a7f65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Note that the targets are the token IDs, which also represent the index positions in the logits tensor that we want to maximize.\n",
    "\n",
    "The cross_entropy function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11069fc5-f8d2-409c-a4f0-0edfac65ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6697)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136de6d-e06a-4d89-9b1c-634bab365615",
   "metadata": {},
   "source": [
    "### 4.4.3 Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5814160-9401-4db6-9c3b-f4e0e3d39887",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>From Loss to an Interpretable Metric: Perplexity</b><br>\n",
    "    A concept directly related to cross-entropy is <b>perplexity</b>. It is simply the exponential of the cross-entropy loss:\n",
    "    <br><br>\n",
    "    $$ \\text{Perplexity} = e^{\\text{Cross-Entropy Loss}} $$\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb47553e-e678-4dc0-b4af-7bbd2b417320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(43031.3984)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340aa3c7-874e-4210-837b-eb2479a9691b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The **perplexity** is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that would be 43,031 words or tokens).\n",
    "\n",
    "In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset.\n",
    "\n",
    "Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
