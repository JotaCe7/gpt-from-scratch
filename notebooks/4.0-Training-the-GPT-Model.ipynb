{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff64fec-d916-4581-9720-9b2dd3d8963b",
   "metadata": {},
   "source": [
    "# Chapter 4: Training a GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80d7fe-ca02-40c7-b3ce-857edc6fabeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In the previous chapters, we successfully designed and built every architectural component of a GPT model from scratch. We now have a complete, functional `GPTModel` class ready to be used.\n",
    "\n",
    "However, the model currently has randomly initialized weights and knows nothing about language. In this chapter, we will take the crucial next step: **training the model**. We will feed it a text corpus, calculate its performance using a loss function, and iteratively update its weights to teach it to generate coherent text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c245b-1326-4703-9607-69aa347a2a81",
   "metadata": {},
   "source": [
    "## 4.1 Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabaddef-e0c9-42ab-9c3d-22641903c99c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We'll begin by importing the necessary libraries and the `GPTModel` we finalized in the last chapter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3403fa84-e80f-492a-aeaf-250bac91bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library and third-party imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# --- Add Project Root to Python Path ---\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "current_notebook_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# --- Imports from your `src` package ---\n",
    "from src.data_loader import create_dataloader\n",
    "from src.utils import load_data\n",
    "from src.model import GPTModel\n",
    "\n",
    "# We will use the same configuration dictionary\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428a27c-07f5-46e8-a95f-40f0d5ae4c0c",
   "metadata": {},
   "source": [
    "## 4.2 Generating Text with the Untrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da85e3f-1cc9-4055-a18a-cf7bb5df3e53",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Before we train the model, let's see what it produces with its random, untrained weights. To do this, we need a function that can perform an **autoregressive** generation loop. This process involves feeding the model an initial context, predicting the next token, adding that token back to the context, and repeating the process.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7fa2bf-a2d5-4155-aad3-f8276162c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Crop current context if exceeds the supported context size\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions from the model\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "\n",
    "         # Focus only on the prediction for the very last token in the sequence\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the token ID with the highest probability (greedy decoding)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append the new token ID to therunning sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86a846-962e-4ed8-a2e2-9e6a08200c04",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The `generate_text_simple` function implements **greedy decoding**. In this autoregressive process, the model repeatedly predicts the single most likely next token, appends it to the sequence, and feeds the new sequence back into the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd40bb-5d4b-4b8e-a937-aacbd8b20bd8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A Note on `softmax` and `argmax`</b><br>\n",
    "  \n",
    "  In our function, we include a `softmax` step to convert the model's output scores (logits) into probabilities before finding the most likely token with `argmax`.\n",
    "\n",
    "  However, since `softmax` doesn't change the order of the scores, applying `argmax` directly to the `logits` would produce the exact same result. We include the step here to clearly illustrate the full process of generating probabilities, but it is technically redundant for greedy decoding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd5e55-d4da-4160-ae00-63f05562717b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Later in this chapter, when we will implement the GPT training code, we will also introduce additional sampling techniques where we modify the softmax outputs such that the model doesn't always select the most likely token, which introduces variability and creativity in the generated text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210e2fee-d34e-4a40-a60a-f10d94c42572",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, let's test our `generate_text_simple` function. We'll provide it with the starting context \"Hello, I am\" by first encoding the string into a batch of token IDs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31d128b-b3a3-4731-8074-d115fa71a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "Encoded tensor shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input\n",
    "start_context = \"Hello, I am\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded:\", encoded)\n",
    "print(\"Encoded tensor shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e18ab9-0c50-4c3c-a388-e60eb24c5522",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we'll set the model to evaluation mode with `model.eval()`. This disables random components like dropout that are only used during training. We can then call our function to generate new tokens from the starting context.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380b5437-b208-4621-8403-b506f4069d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiate the Model ---\n",
    "torch.manual_seed(100)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model instantiated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c3fa0-7596-47d7-bc4e-96f499c65a55",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's generate text using our `generate_text_simple` function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a686c8-7fe8-4b6a-8596-520c5e5b1b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716,  1908, 41574, 14356, 11426, 42884, 32296]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "output_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", output_ids)\n",
    "print(\"Output length:\", len(output_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647398c-cc88-4f9b-92fc-23e7adb54279",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Finally, we use the tokenizer's `.decode()` method to convert the output token IDs back into a readable string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333ee864-b9a4-48bc-a480-ca97cd8c7e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded text: 'Hello, I am sent agitated AW telephone Tomas shroud'\n"
     ]
    }
   ],
   "source": [
    "# Decode the output\n",
    "decoded_text = tokenizer.decode(output_ids.squeeze(0).tolist())\n",
    "print(f\"\\nDecoded text: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd523c-a439-404a-bf55-544edd3509c2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "  <b>Why is the output gibberish?</b><br>\n",
    "  \n",
    "  As we can see, the generated text is incoherent. This is the correct and expected result at this stage.\n",
    "\n",
    "  The reason is that our model is completely **untrained**. Its weights are still the random values they were initialized with. It has not yet learned any patterns of the English language. This demonstration perfectly illustrates *why* we need to train the model. In the next sections, we will prepare a dataset and implement a training loop to do just that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e33c0-e19a-4358-92e8-a1252db2d4d2",
   "metadata": {},
   "source": [
    "## 4.3 Evaluating Generative Text Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed81d31-a18f-4bd2-808a-4c33e239a775",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous section, we saw that our untrained model produces incoherent gibberish. While we can see this qualitatively, we need a quantitative way to measure the model's performance. How do we capture \"good text\" versus \"bad text\" in a number that we can track and optimize?\n",
    "\n",
    "The answer is to use a **loss function**. For language models that predict next-token probabilities, the standard metric is **cross-entropy loss**. It measures how \"surprised\" the model is by the true next token; a lower loss means the model's predictions are closer to reality. A related, more interpretable metric is **perplexity**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde5b01-788f-4fdd-92ad-08005a691694",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we can calculate these metrics, we need to prepare our workspace. This involves two main setup steps:\n",
    "\n",
    "1.  **Initialize the Model:** We will instantiate a `GPTModel` using our configuration with a `context_length` of 256. Using a smaller context size (compared to the original GPT-2's 1024) reduces the computational requirements, making the examples in this chapter accessible on a standard laptop.\n",
    "\n",
    "2.  **Define Helper Functions:** We will define two convenience functions, `text_to_token_ids` and `token_ids_to_text`. These utilities will make it easier to convert back and forth between text and the model's numerical token IDs throughout our analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d72d621b-f5a7-4adf-ae35-25e8619db674",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"dropout_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(100)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c1d1cf-1d40-47a3-a47f-dc843c16c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0841ce7-4827-435d-a0a4-a15ed26a7826",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "With our model initialized and helper functions defined, we can now perform our first end-to-end text generation. We will provide the model with the starting context \"Every effort moves you\" and use the `generate_text_simple` function to have it autoregressively generate the next 10 tokens.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb6848-7f89-48d1-809f-73fc07acf9dd",
   "metadata": {},
   "source": [
    "start_context = \"Every effort moves yosu\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_contexta2, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb3081-b469-4a0d-bfc9-2949d80c177a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see above, the model does not produce good text because it has not been trained yet.\n",
    "\n",
    "**How do we measure or capture what \"good text\" is, in a numeric form, to track it during training?**\n",
    "\n",
    "The next subsection introduces metrics to calculate a loss metric for the generated output that we can use to measure the training progress.\n",
    "\n",
    "The next chapters on finetuning LLMs will also introduce additional ways to neasure model quality.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d62330-d204-48ba-ac3c-bbeee69a693e",
   "metadata": {},
   "source": [
    "## 4.4 Calculating the Text Generation Loss: Cross-Entropy and Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76611d-820f-4ec6-b456-782e5a45be80",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Suppose we have an inputs tensor containing the token IDs for 2 training example (rows).\n",
    "\n",
    "Corresponding to the inputs, the targets contain the desired token IDs that we want the model to generate.\n",
    "\n",
    "Notice that the targets are the inputs shifted by 1 position, as we explained in *Chapter 1* when we implemented the data loader.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363a6635-9bdb-46c6-b3f1-5e02eba64b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b2634-1209-4bae-ad52-ca4529789d46",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each.\n",
    "\n",
    "Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary.\n",
    "\n",
    "Applying the softmax function, we can turn the logits into a tensor of the same dimension containing probability scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709f5674-22d4-4b26-a02c-415d6e46e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0ad7-bb08-4097-866c-81678f60c830",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As discussed in the previous section, we can apply the argmax function to convert theprobability scores into predicted token IDs.\n",
    "\n",
    "The softmax function above produced a 50,257-dimensional vector for each token; the argmax function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token.\n",
    "\n",
    "Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "303b4a4b-1505-4bfe-85d4-f19537a6fbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[24926],\n",
      "         [49334],\n",
      "         [45732]],\n",
      "\n",
      "        [[ 3401],\n",
      "         [13257],\n",
      "         [46567]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54694150-4e7a-4991-a0a4-4501e4a8ea21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens.\n",
    "\n",
    "That's because the model wasn't trained yet.\n",
    "\n",
    "To train the model, we need to know how far it is away from the correct predictions (targets).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4951573b-bf87-4064-b78f-b5ae0aca7113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  huhbrainer Harding\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac37f667-3331-4330-8052-56ac177e84d1",
   "metadata": {},
   "source": [
    "### 4.4.1 Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a9212-5474-4ee5-ba44-a031cacf7d11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The token probabilities corresponding to the target indices are as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2132b105-c4b5-435a-85eb-0134fc9ed32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([9.0643e-06, 6.8038e-06, 6.5535e-05])\n",
      "Text 2: tensor([1.0236e-04, 2.3294e-05, 1.6344e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6d381-7b7a-4b11-b2e3-5cb5acf6eba3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We want to maximize all these values, bringing them close to a probability of 1.\n",
    "\n",
    "In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e65405f-5023-419a-ba7e-efd56d94f0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.6112, -11.8980,  -9.6329,  -9.1870, -10.6673, -11.0216])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc60899-fe26-4e19-a4fd-55dd9093710c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we compute the average log probability:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb12e499-67a9-4c86-a984-8984246272c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.6697)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probabililty for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b320a-0555-4d07-86b1-149129c2b13f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The goal is to make this average log probability as large as possible by optimizing the model weights.\n",
    "\n",
    "Due to the log, the largest possible value is 0, and we are currently far away from 0.\n",
    "\n",
    "In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.6697 so that it approaches 0, we would maximize 10.6697 so that it approaches 0.\n",
    "\n",
    "The negative value of -10.6697, i.e., 10.6697, is also called **cross-entropy loss** in deep learning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ba7972-4829-4b69-ac5f-79e6b4186e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6697)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38045d8e-9d09-4c27-afb1-5411c1380d07",
   "metadata": {},
   "source": [
    "### 4.4.2 Cross-Entropy Loss Using PyTorch's Optimized Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6dd5c7-2cf3-4f01-97ee-f26c1e904929",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Thankfully, we don't have to do all that math manually. PyTorch provides a highly optimized `cross_entropy` function that performs the softmax and log-probability calculations for us.\n",
    "\n",
    "Before we can use it, we need to reshape our `logits` and `targets` tensors. The function expects the `logits` to have the shape `(N, C)` where `C` is the number of classes (our `vocab_size`), so we will flatten the batch and token dimensions together.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30163929-a78a-48a4-8ee1-30d759241d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60c38b-dc74-4a13-920b-1319d48f343c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For the **cross_entropy** function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98ca5f8a-d285-4421-8c37-8014732ecdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits shape: torch.Size([6, 50257])\n",
      "Flattened targets shape: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits shape:\", logits_flat.shape)\n",
    "print(\"Flattened targets shape:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390aaab1-62e0-432e-9be0-a01cd46a7f65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Note that the targets are the token IDs, which also represent the index positions in the logits tensor that we want to maximize.\n",
    "\n",
    "The cross_entropy function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11069fc5-f8d2-409c-a4f0-0edfac65ee4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.6697)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136de6d-e06a-4d89-9b1c-634bab365615",
   "metadata": {},
   "source": [
    "### 4.4.3 Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5814160-9401-4db6-9c3b-f4e0e3d39887",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>From Loss to an Interpretable Metric: Perplexity</b><br>\n",
    "    A concept directly related to cross-entropy is <b>perplexity</b>. It is simply the exponential of the cross-entropy loss:\n",
    "    <br><br>\n",
    "    $$ \\text{Perplexity} = e^{\\text{Cross-Entropy Loss}} $$\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb47553e-e678-4dc0-b4af-7bbd2b417320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(43031.3984)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340aa3c7-874e-4210-837b-eb2479a9691b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The **perplexity** is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that would be 43,031 words or tokens).\n",
    "\n",
    "In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset.\n",
    "\n",
    "Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58679b3-ebc6-4627-825d-123726da0d21",
   "metadata": {},
   "source": [
    "## 4.5 Setting Up the Dataset and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5163b0-25a4-4579-b6b6-ea19cb9c7c49",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "For this project, we will train our LLM on a relatively small dataset: a single short story. There are several practical reasons for this choice:\n",
    "\n",
    "<ul>\n",
    "    <li><b>Accessibility:</b> We can run all the code examples in a few minutes on a standard laptop without needing a powerful GPU.</li>\n",
    "    <li><b>Speed:</b> The training process finishes quickly, whihc is ideal for experimentation.</li>\n",
    "    <li><b>Licensing:</b> The text is in the public domain, so it can be included in this repository without any usage rights issues.</li>\n",
    "</ul>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>A Note on Real-World Scale</b><br>\n",
    "    While our dataset has about 5,000 tokens, it's important to understand the scale of real-world models. For example, training Llama 2 (7B) required <b>2 trillion tokens</b> and approximately <b>$700,000</b> in cloud computing costs.\n",
    "</div>\n",
    "\n",
    "With our small dataset, the next step is to divide it into a training set (for the model to learn from) and a validation set (to check the model's performance on unseen data).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a0ced-2aba-4cab-863e-55135ec9ccad",
   "metadata": {},
   "source": [
    "### 4.5.1 Preparing the DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859de87-cdce-4c6a-8566-9c6711286854",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We will use the same text corpus (\"The Verdict\") and the same datga loader from our previous work. The process involves:\n",
    "\n",
    "1. Loading the text file.\n",
    "2. Splitting it into a 90% training set and 10% validation set.\n",
    "3. Using our `GPTDatasetV1` and `create_dataloader` class and function to create data loaders that will feed the data to our model in batches.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31738941-077c-4aa7-8c83-3ed40097f267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../data/the-verdict.txt' already exists. Loading fomr disk...\n",
      "Load complete.\n",
      "\n",
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the data\n",
    "file_path = \"../data/the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/JotaCe7/llm-text-preprocessing/main/data/the-verdict.txt\"\n",
    "text_data = load_data(file_path=file_path, url=url)\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"\\nCharacters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4feca2f5-b41d-454a-92fe-296bd5f18bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters in training set: 18,431\n",
      "Characters in validation set: 2,048\n"
     ]
    }
   ],
   "source": [
    "# 2. Split the data into training and validation sets\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "print(f\"Characters in training set: {len(train_data):,}\")\n",
    "print(f\"Characters in validation set: {len(val_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590cee40-4d8f-4e82-bf99-73b7dfd6fc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "9 tarining batches\n",
      "1 validation batches\n",
      "\n",
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "# 3. Create the data loaders\n",
    "torch.manual_seed(100)\n",
    "\n",
    "train_loader = create_dataloader(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# Optional check to validate datakiader were created correctly\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(f\"{len(train_loader)} tarining batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"\\nTraining tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01ec8b2a-f812-48cc-8e5c-b3ca9686b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d3e87-bfdd-4895-9fe0-af70efd204ab",
   "metadata": {},
   "source": [
    "### 4.5.2 Defining the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46189b3-5e83-48a9-8544-9ac1e7364b9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, we need functions to calculate the loss. We will use **cross-entropy loss**, which is the standard for language models. The `calc_loss_batch` function computes the loss for a single batch, and `calc_loss_loader` will average the loss over multiple batches from a data loader.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02d6c5a1-ede9-4a67-90f7-a7ad23931598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i <num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64816873-0400-4fc1-b1a7-b3e83b495918",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now, let's calculate the initial loss on our training and validation sets before any training has occurred. This gives us a baseline to measure our progress against\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c18855-924c-4ab5-8eab-521734cf69c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Training loss: 10.980\n",
      "Initial Validation loss: 10.959\n"
     ]
    }
   ],
   "source": [
    "# Set the devive\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Calculte initial losses\n",
    "with torch.no_grad(): # Disable gradient tracking for this calculation\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(f\"Initial Training loss: {train_loss:.3f}\")\n",
    "print(f\"Initial Validation loss: {val_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602600d-aa9b-46a4-bed9-e80497b94dde",
   "metadata": {},
   "source": [
    "## 4.6 Implementing and Running the Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ee4585-41f9-46d3-a38c-d05bf644d61b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "With our data loaders and loss functions ready, we have all the preerquisites for training. We will now define the main thaining function that iterate over the data, computes the loss, and updates the model weights's.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fab333-9ecd-4f0c-9083-85f44df98854",
   "metadata": {},
   "source": [
    "### 4.6.1 Defining the Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2098e1-7350-4fa2-b6bf-f063aa996ef6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We need three helper functions for our training process:\n",
    "\n",
    "* **`train_model_simple:`** The main function that contains the training loop over multiple epochs.\n",
    "* **`evaluate_model:`** A function to periodically calculate the loss on the training and validation sets to monitor performance.\n",
    "* **`generate_and_print_sample:`** A function to generate sample text after each epoch so we can qualitatively see how the model is improving.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c4cdddba-80da-4716-b073-fe0c8cbe9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep. {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:3f}, Val loss {val_loss:3f}\")\n",
    "\n",
    "                # Print a sample text after each epoch\n",
    "                generate_and_print_sample(\n",
    "                    model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8dbc97-556c-44ca-9db4-b1029d172fab",
   "metadata": {},
   "source": [
    "### 4.6.2 Kicking Off the Trainig Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f3d61-85de-451c-8aae-626c0e07b48e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "With all our helper function defined, we are finally ready to train our GPT model! We will instantiate a new model and a and an AddamW optimizer, then call our main training function for 10 epochs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "486dd718-bee5-4a41-ad85-abd00f68792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 (Step 000000): Train loss 9.912392, Val loss 10.146250\n",
      "Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the, the the the the the the the the the the the\n",
      "Ep. 1 (Step 000005): Train loss 8.021554, Val loss 8.372361\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep. 2 (Step 000010): Train loss 6.659008, Val loss 7.080389\n",
      "Every effort moves you.                                                 \n",
      "Ep. 2 (Step 000015): Train loss 6.045608, Val loss 6.617142\n",
      "Every effort moves you, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the, the\n",
      "Ep. 3 (Step 000020): Train loss 5.465610, Val loss 6.449451\n",
      "Every effort moves you, and, and\", and, and, and, and I had the, and, and I of the, and, and, and, and, and, and, and, and, and, and, and, and, and of\n",
      "Ep. 3 (Step 000025): Train loss 5.359938, Val loss 6.440866\n",
      "Every effort moves you, and to the                                              \n",
      "Ep. 4 (Step 000030): Train loss 5.256047, Val loss 6.383637\n",
      "Every effort moves you to the to the picture to the I had the picture--I a                                    \n",
      "Ep. 4 (Step 000035): Train loss 4.850055, Val loss 6.416106\n",
      "Every effort moves you to the to the picture a a. Gisburn--as a little a little that of the picture--and I had the picture to the picture--as was a little--I a little it--as, and \" \" of the picture of\n",
      "Ep. 5 (Step 000040): Train loss 4.336968, Val loss 6.278005\n",
      "Every effort moves you to see to see a little--I had the picture--as and in the picture to see a little--I, and I had to see it--the the picture to see it--I he had the picture, and I had been to see\n",
      "Ep. 6 (Step 000045): Train loss 4.099913, Val loss 6.354907\n",
      "Every effort moves you know that, I felt I felt as I felt nervous and Mrs. \"I felt he had been--and I felt to see it was a little his pictures--I felt--as, I saw that, I felt, I felt I felt\n",
      "Ep. 6 (Step 000050): Train loss 3.118167, Val loss 6.188000\n",
      "Every effort moves you know that, and in the picture. Gisburn--as the end of the fact, and I had been, and his pictures to the end of his pictures to have been--as if he had been his pictures--his, in the picture\n",
      "Ep. 7 (Step 000055): Train loss 3.017391, Val loss 6.223173\n",
      "Every effort moves you know that, and I was a     \"I must--I didn't--I looked up, I felt to see a smile behind his close grayish beard--as if he had the secret, and were, and I had\n",
      "Ep. 7 (Step 000060): Train loss 2.489498, Val loss 6.250611\n",
      "Every effort moves you of the inevitable garlanded frame. The mere outline of the frame. Gisburn had always's past!  \"I didn't you know back the window-curtains, I had the secret, the fact--and I had\n",
      "Ep. 8 (Step 000065): Train loss 2.141709, Val loss 6.217545\n",
      "Every effort moves you know that, on a little wild--I felt nervous and uncertain.  \"Once, when I looked up, I felt to see a smile behind his close grayish beard--as if he had the secret, and were amusing himself by his\n",
      "Ep. 8 (Step 000070): Train loss 1.734339, Val loss 6.253887\n",
      "Every effort moves you know that mighty up-stream stroke.    \"I looked up and I felt him, and Mrs. \"I must a smile behind his close grayish beard--as if he had the donkey.  \"I didn't\n",
      "Ep. 9 (Step 000075): Train loss 1.412538, Val loss 6.289547\n",
      "Every effort moves you?\"                          I a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" he had\n",
      "Ep. 9 (Step 000080): Train loss 1.114690, Val loss 6.318080\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and Mrs. Stroud so when he had the me to me to the cigars you like.\"  He placed them at my elbow and as I had been the \"strongest,\" as his\n",
      "Ep. 10 (Step 000085): Train loss 0.895781, Val loss 6.393708\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to the fact that, when, on a later day, I had again run over from his ease--because he didn't want\n",
      "Training completed in 0.618973 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(100)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11244af8-8ed5-4750-a5ff-ead4eb39cc9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see, based on the results printed during the training, the training loss improves drastically, starting with a value of 9.912392 and converging to 0.895781.\n",
    "\n",
    "The language sikills of the model have improvded quite a lot too. In the beginning, the model is only able to append commas to the start context (\"Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\") or repeat the worl \"the\"/\"and\".\n",
    "\n",
    "At the end of the training, it can generate grammatically correct text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b10c62-3fc8-4710-a8c4-f545f9f55721",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Similar to the training set loss, we can see that the validation set loss starts high (10.146250) and decreases during the training.\n",
    "\n",
    "However, it never becomes as small as the training set loss and remains at 6.393708 after the 10th epoch.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cac43b1-d0b3-46e6-9b6b-7f07f0ed5e16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A Note on the AdamW Optimizer</b><br>\n",
    "  \n",
    "  The <b>AdamW optimizer</b> is a modern and powerful algorithm that has become a standard and highly recommended choice for training large language models like GPT. It enhances the popular Adam optimizer with an improved implementation of weight decay.\n",
    "\n",
    "  <ul>\n",
    "    <li><b>Adaptive Learning Rates:</b> Like Adam, it maintains a unique, adaptive learning rate for every single weight in the model. This means it can learn faster on some parameters and more cautiously on others.</li>\n",
    "    <li><b>Momentum:</b> It uses momentum, which is like a memory of past gradients, to help it move more consistently in the right direction during optimization.</li>\n",
    "    <li><b>Improved Weight Decay (\"W\"):</b> The 'W' in AdamW stands for Weight Decay. It fixes a flaw in the original Adam's implementation of weight decay (a regularization technique that helps prevent overfitting). This decoupled approach often leads to better model performance and generalization.</li>\n",
    "  </ul>\n",
    "  \n",
    "  Because of its robustness and effectiveness, AdamW is now the default optimizer for most transformer-based projects.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16166172-8d78-4188-894f-2a97dd5f6b5d",
   "metadata": {},
   "source": [
    "### 4.6.3 Analyzing the Training Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f53dbc-b20b-4c00-bfde-ba24809c232b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "After training is complete, the best way to understand how the model learned is to plot the training and validation loss curves over time. A decreasing loss indicates that the model is learning the patterns in the text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b3cb172-b250-4e92-a20d-ef308098b7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_losses(epochs_seen, tokens_seen,train_losses, val_losses):\n",
    "    \"\"\"Plots the training and validation loss curves.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot training and validation losses against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on bottom x-axis\n",
    "\n",
    "    # Creeate a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny() # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0) # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True)) # only show integer labels on upper x-axis\n",
    "\n",
    "    fig.tight_layout() # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4df9a2-69c7-405a-96f8-66caea1b7798",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>Analysis: Interpreting the Loss Curves and Overfitting</b><br>\n",
    "    \n",
    "  The plot clearly shows both the training and validation losses decreasing sharply, which means our model is successfully learning from the data.\n",
    "  \n",
    "  However, notice the growing gap between the training loss and the validation loss. When the validation loss is significantly higher than the training loss (or even starts to increase), it's a classic sign of <b>overfitting</b>. The model is beginning to memorize the small training dataset instead of learning general language patterns, which we can confirm by searching for the generated text snippets, such as `\"I had again run over from\"` in the \"The Verdict\" text file. This is expected because our dataset is tiny and we are training for several epochs. In a real-world scenario with a massive dataset, we would typically only train for a single epoch.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc88ed-2baf-4418-929a-46c6d92f3a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
