{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed5b720-10cf-4972-93a4-5096d49af9e5",
   "metadata": {},
   "source": [
    "# Chapte 1: LLM Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2f1f3-89ff-433b-9772-5ff502e71da4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Welcome to the first step in building a Large Language Model! Before a model can learn from text, we must convert that text into a numerical format it can understand. This process is called **text preprocessing**.\n",
    "\n",
    "In this chapter, we will build a complete pipeline that takes a raw text file as input and produces batches of token embeddings as output, ready to be fed into a transformer model. We will cover:\n",
    "<ul>\n",
    "    <li>Simple tokenization using regular expressions.</li>\n",
    "    <li>The industry-standard <b>Byte-Pair Encoding (BPE)</b> tokenizer.</li>\n",
    "    <li>Creating input-target pairs for autoregressive training.</li>\n",
    "    <li>Using PyTorch's `Dataset` and `DataLoader` to create batches.</li>\n",
    "    <li>Generating final token and positional embeddings.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd13af-84a3-4620-b297-3c9ef475cdcf",
   "metadata": {},
   "source": [
    "## 1.1 Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82878542-cae1-4d93-b145-c69601dd0b64",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We'll begin by importing all the necessary libraries and modules that we will use throughout this chapter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a419f6-7406-46f2-98c6-396ca99145da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in e:\\venvs\\building-llm\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in e:\\venvs\\building-llm\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in e:\\venvs\\building-llm\\lib\\site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\venvs\\building-llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\venvs\\building-llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\venvs\\building-llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\venvs\\building-llm\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import importlib\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9504a9d-4649-4a5f-812a-ed77a62ff9b4",
   "metadata": {},
   "source": [
    "## 1.2 Reading the Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f6993-cfc5-4f9e-817c-8942de7d3b29",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "To begin our preprocessing pipeline, we first need to load our raw text corpus. We'll define a reusable helper function, `load_data`, that can download a file from a URL if it doesn't already exist locally. This will make our notebook self-contained and easy for anyone to run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c9a3a9-b0b2-4306-95b9-8bc5631793c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, url = None):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Downloading data from {url}...\")\n",
    "        with urllib.request.urlopen(url) as resposne:\n",
    "            text_data = resposne.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(f\"File '{file_path}' already exists. Loading fomr disk...\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "        print(\"Load complete.\")\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdee1b7-6d8c-4078-9c86-5d8340420395",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Our dataset for this project will be the complete text of **\"The Verdict,\"** a short story by the Pulitzer Prize-winning American writer **Edith Wharton**, first published in 1908.\n",
    "\n",
    "We are using this text for three main reasons:\n",
    "* It is in the public domain, so we can use it freely.\n",
    "* It is written in a rich, classic style, providing interesting vocabulary and sentence structures.\n",
    "* Its size is small enough to allow for quick processing and training on a standard laptop.\n",
    "  \n",
    "Now, let's use this function to load the text and inspect its contents to ensure it loaded correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d0d4987-9edf-4305-8a06-589907639f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../data/the-verdict.txt' already exists. Loading fomr disk...\n",
      "Load complete.\n",
      "\n",
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"../data/the-verdict.txt\"\n",
    "URL = \"https://raw.githubusercontent.com/JotaCe7/llm-text-preprocessing/main/data/the-verdict.txt\"\n",
    "\n",
    "raw_text = load_data(file_path=FILE_PATH, url=URL)\n",
    "\n",
    "print(\"\\nTotal number of characters:\", len(raw_text))\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30620ff6-8f8b-4890-bcd4-f1a971a33eac",
   "metadata": {},
   "source": [
    "## 1.3 A Simple Word Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92bacdf-1044-46ca-a72a-623745288af1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The first step in preprocessing is **tokenization**, splitting the raw text into smaller units, or \"tokens.\" A simple first approach is to treat every word and piece of punctuation as a token. We can achieve this using Python's regular expression library.\n",
    "\n",
    "For this example, we are ignoring whitespaces. For certain application we will need to keep them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25903fa1-73ab-4ff5-a482-935d469ed4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8bfed-00fd-4e11-a37c-6f37e7401210",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.3.1 Building a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7cb376-1e02-4117-9706-85e34a8d8036",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Now that we have a list of all tokens, we need to convert them into integers. We do this by first creating a **vocabulary**, a sorted list of all unique tokens in our text. We then create a dictionary that maps each unique token to a unique integer ID.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a337bfa3-f2ab-4cb0-84a3-1efa28d5861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 1130\n",
      "\n",
      "Let's print 15 first entries of our vocabulary:\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Size of our vocabulary:\",vocab_size)\n",
    "\n",
    "vocab = {token:id for id, token in enumerate(all_words)}\n",
    "print(\"\\nLet's print 15 first entries of our vocabulary:\")\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 14:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656be4b3-4f3f-409f-ae33-4b8c85dddb98",
   "metadata": {},
   "source": [
    "### 1.3.2 Creating a Tokenizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b5967-4731-4c65-b334-20878e40f7c4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Now that we understand the process of creating a vocabulary and mapping tokens to integer IDs, let's encapsulate this logic into a complete `SimpleTokenizerV1` class.\n",
    "\n",
    "This class will handle the full round-trip process:\n",
    "* The **`encode()`** method will take a raw text string, split it into tokens using our regex, and convert it into a list of token IDs based on the vocabulary.\n",
    "* The **`decode()`** method will perform the reverse operation, taking a list of token IDs and converting it back into a human-readable text string, handling punctuation spacing correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9498fe9e-3db7-4ae9-8ded-c061952ef471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.encoder = vocab\n",
    "        self.decoder = {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.encoder[item] for item in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.decoder[id] for id in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee0b2f-0731-4cd5-827b-ed64995b4020",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For demonstration, let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from Edith Wharton's short story to try it out in practice:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97091222-3256-4293-9f16-656461b0d314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 74, 5, 241, 58, 0, 1, 53, 851, 7, 56, 1077, 115, 899, 722, 115, 361, 6, 156, 726, 1015, 361, 5, 923, 568, 988, 815, 1044, 115, 1072, 7, 1, 23, 58, 6, 115, 89, 0, 1, 53, 300, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizerv1 = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"Oh, by Jove!\" I said.\n",
    "\n",
    "It was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.\n",
    "\n",
    "\"By Jove--a Stroud!\" I cried.\"\"\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecbe2b-14ab-4afa-8a71-eb6d8dfd5004",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, let's see if we can turn these token IDs back into text using the decode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb63e011-b33f-4aeb-8ee6-6eff6911a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" Oh, by Jove!\" I said. It was a sketch of a donkey -- an old tired donkey, standing in the rain under a wall.\" By Jove -- a Stroud!\" I cried.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizerv1.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba678e51-99ea-4a37-8761-24bc3e20a0ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>The \"Unknown Token\" Problem</b><br>\n",
    "\n",
    "A major limitation of this simple vocabulary is that it only knows about words present in our training text. If it encounters a new word, it will fail. This is known as the **\"out-of-vocabulary\"** or **\"unknown token\" problem**.\n",
    "\n",
    "This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0015b0e-4588-4f10-ae6d-c0347dcbf2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "ids = tokenizerv1.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e0d6f-92fd-4b4c-bbc8-8769e50fd1b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.3.3 Special Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f4df6-6280-4c35-a1ca-bc31cead1b0c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set. \n",
    "\n",
    "In this section, we will modify this tokenizer to handle unknown words.\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, **<|unk|>** and **<|endoftext|>**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee477d4f-b1d1-4181-8baf-5059e6bea883",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "We can modify the tokenizer to use an **<|unk|>** token if it encounters a word that is not part of the vocabulary. \n",
    "\n",
    "Furthermore, we add a token between unrelated texts (**<|endoftext|>**). \n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c83e81-7925-4a0b-8e4d-c0420092e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of our vocabulary: 1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "print(\"Size of our vocabulary:\",len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4dd171-fc87-43b9-b7b1-82ef79126290",
   "metadata": {},
   "source": [
    "### 1.3.4 Creating a Tokenizer Class with Context Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3df6b0-20d8-422d-9d39-941f0ba8654c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "A simple text tokenizer that handles unknown words\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ef4498-066e-49a6-97e9-d93a8c7222cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.encoder = vocab\n",
    "        self.decoder = {id:token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.;:?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        preprocessed = [\n",
    "            item if item in self.encoder\n",
    "            else \"<|unk|>\" for item in preprocessed # Replace unknown words by <|unk|>\n",
    "        ]\n",
    "        ids = [self.encoder[token] for token in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.decoder[id] for id in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.;:?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0864b74-66d8-43ea-8685-aedca69d686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 560, 169, 1126, 10]\n",
      "<|unk|>, how are you?\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, how are you?\"\n",
    "tokenizerv2 = SimpleTokenizerV2(vocab)\n",
    "ids = tokenizerv2.encode(text)\n",
    "print(ids)\n",
    "print(tokenizerv2.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa090fc2-c7cb-4a9e-87b2-d7dc0724878d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Based on the detokenized output, we can know that \"Hello\" is not part of the vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c2f82-a8e2-4d93-9702-c1e37f437f11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Depending on the LLM, some researchers also consider additional special tokens such as the following.\n",
    "\n",
    "* **[BOS] (Beginning Of Sequence):** This token marks the start of a text.\n",
    "\n",
    "* **[EOS] (End Of Sequence):** This token is positioned at the end of a text. It is especially useful when concatenating multiple unrelated documents.\n",
    "\n",
    "* **[PAD] (Padding):** To process text in batches, all sequences must have the same length. Shorter texts are extended or \"padded\" to this length using the `[PAD]` token.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e8213-5925-4267-81ac-2b39ce7c157f",
   "metadata": {},
   "source": [
    "## 1.4 The Standard: Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa41543-9de2-4b6a-9920-1590052f5185",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The simple word-based tokenizer we built highlights a major challenge: how to handle words that are not in the vocabulary. Modern LLMs solve this using a powerful technique called **subword tokenization**.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>A Note on GPT's Tokenizer</b><br>\n",
    "  \n",
    "  It's important to note that while we've discussed special tokens like `[BOS]` and `[PAD]`, the actual tokenizer used for GPT models is simpler in one way and more complex in another. \n",
    "  \n",
    "  It only requires a single special token, <code>&lt;|endoftext|&gt;</code>, to signal the end of a text passage. For all other cases, instead of using an <code>&lt;|unk|&gt;</code> token for unknown words, it uses the most popular and powerful subword algorithm called <b>Byte-Pair Encoding (BPE)</b>, which breaks down any word it doesn't know into smaller, known subword units.\n",
    "</div>\n",
    "\n",
    "We will use OpenAI's high-performance `tiktoken` library, which provides a pre-trained BPE tokenizer used in GPT-2. The `encode()` and `decode()` methods in `tiktoken` work very similarly to the simple tokenizer we built. Let's see it in action.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669977e3-587d-462a-9875-1158c256231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb5ccb40-14cf-499c-8f93-4cb488d7e498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n",
      "\n",
      "Encoded IDs: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "\n",
      "Decoded text: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# Original text\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces \"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "print(\"Original text:\", text)\n",
    "\n",
    "# Encoding sample text\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(\"\\nEncoded IDs:\", integers)\n",
    "\n",
    "# Decoding back to text\n",
    "strings = tokenizer.decode(integers)\n",
    "print(\"\\nDecoded text:\", strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a52e8d-776c-4a5f-ad04-79bde35f8d1e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The version of the BPE Tokenizer used by GPT-2 has a total vocabulary size of **50,257**, with `<|endoftext|>` being assigned the largest token ID (50256), as we can see in the Encoded IDs above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c70697-3139-4f8c-833c-8a6e6c300660",
   "metadata": {},
   "source": [
    "#### Vocabulary size comparison for different GPT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b45dca7-6a4a-4717-91a3-cbc183dd4f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50257\n",
      "The vocabulary size for GPT3 is: 50281\n",
      "The vocabulary size for GPT4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
    "}\n",
    "\n",
    "# Get the vocabulary size for each encoding\n",
    "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "# Print the vocabulary sizes\n",
    "for model, size in vocab_sizes.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf77ca-2b6f-4f68-8c71-b1fa4435e40a",
   "metadata": {},
   "source": [
    "## 1.5 Preparing Data Batches For Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9c030-ce78-48ab-b02b-0a0cfd89343e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The fundamental task of a language model like GPT is to predict the next token in a sequence. To train it, we need to structure our text data into `(input, target)` pairs. The **input (`x`)** will be a chunk of text, and the **target (`y`)** will be the same chunk, but shifted one position to the right.\n",
    "\n",
    "This setup teaches the model: \"When you see this sequence of tokens, the next token should be *this* one.\"\n",
    "\n",
    "We will do this using a sliding window approach and then use PyTorch's `Dataset` and `DataLoader` classes to create batches of data efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751d2258-060f-4f84-b9cd-abb3e669d83b",
   "metadata": {},
   "source": [
    "### 1.5.1 Creating Input-Target Pairs for Autoregressive Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db11ec-97f4-45e3-a818-34203b5788c4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous section:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07ca2d64-b868-49df-847a-f1fee002296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text was tokenized into 5145 tokens.\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(f\"The text was tokenized into {len(enc_text)} tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479accd-4d53-4732-af35-d9627c6d0a82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "One of the easiest and most intuitive ways to create the input-target pairs for the next-word prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1 (The context size determines how many tokens are included in the input):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dca10a6-70a2-45c3-be39-52821cbc07e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  (input): [40, 367, 2885, 1464]\n",
      "y (target):     [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 # length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "\n",
    "print(f\"x  (input): {x}\")\n",
    "print(f\"y (target):     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d9156-a8b9-41bf-9c71-50bc0f3c6e7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The input-target pair above actually contains multiple, smaller prediction tasks. For a given input `x`, the model tries to predict the corresponding `y` at each position.\n",
    "\n",
    "The `for` loop below illustrates these individual tasks, both with token IDs and decoded text. Everything to the left of the arrow (`--->`) is the context the model sees, and the token on the right is the token it is being trained to predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b99460cc-93ce-48fa-86cb-840dd950d899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] ---> 367\n",
      "[40, 367] ---> 2885\n",
      "[40, 367, 2885] ---> 1464\n",
      "[40, 367, 2885, 1464] ---> 1807\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_size):\n",
    "    context = enc_text[:i+1]\n",
    "    desired = enc_text[i+1]\n",
    "    print(context, \"--->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4042257-df6a-4b38-bd88-bcf3bb3d5c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --->  H\n",
      "I H ---> AD\n",
      "I HAD --->  always\n",
      "I HAD always --->  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_size):\n",
    "    context = enc_text[:i+1]\n",
    "    desired = enc_text[i+1]\n",
    "    print(tokenizer.decode(context), \"--->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b6b0f-3fcf-4f30-96f9-e366960f4de5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "This manual process is great for understanding a single training example. However, to process our entire text corpus of thousands of tokens efficiently, and turn them into embeddings, we need to automate this \"chunking\" process and create batches of data.\n",
    "\n",
    "In particular, we are interested in returning two PyTorch tensors (multidimensional arrays): an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a118e-aca4-4999-a4be-7ab833e5f6af",
   "metadata": {},
   "source": [
    "### 1.5.2 Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35fc41-d02e-4239-9cc6-15cfc61420d8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "First, we create a custom `Dataset` class that will take our tokenized text and use a sliding window to create the input-target chunks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4a2d353-6055-4f0a-836d-590b3b81be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "\n",
    "        # Use a sliding window chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5a776-e8fb-4bd8-854f-b2980a929d55",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Next, to make creating the final `DataLoader` easy, we'll wrap the `GPTDataset` and PyTorch's `DataLoader` in a single convenience function, `create_dataloader`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d27c86f-6748-44d5-a160-cede8a444b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fe8aa-5ab5-47c5-8eab-a7921a2f887a",
   "metadata": {},
   "source": [
    "### 1.5.3 Exploring the Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fde178-8dc4-40c3-bec2-b4cfb310f44f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "With our data loader ready, let's explore the data it produces. We'll start with a `batch_size` of , a `stride` of 1 and `context_length = 4` to clearly see how the sliding window creates overlapping chunks.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1bd2dae-f2a2-4c84-9ffa-84a3d6462161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.7.1+cu128\n",
      "first_batch: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(\"first_batch:\", first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec5851-292f-4b95-b473-a4edcfb7b61e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs.\n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs. \n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least 256.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab581e60-11a7-4ef1-98a9-aced0cbd95db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37da79cd-d1a6-4a88-b71a-ca895128683c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_batch: [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(\"second_batch:\", second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae45046-fc2b-462b-864e-0f23f125472a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "If we compare the first with the second batch, we can see that the second batch's token IDs are shifted by one position compared to the first batch. \n",
    "\n",
    "For example, the second ID in the first batch's input is 367, which is the first ID of the second batch's input. \n",
    "\n",
    "The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f9585-3336-46d0-b90d-ee8bbdda5b40",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Before we move on to creating the embedding vectors from the token IDs, let's have a brief look at how we can use the data loader to sample with a batch size greater than 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3db168a5-77ff-42e0-8700-b3bc37e76e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=8,max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1a98e-1434-40e8-b8ea-3c4f6533f6c3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap could lead to increased overfitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b914927-3799-4008-9ca6-02ac3c9d14c0",
   "metadata": {},
   "source": [
    "## 1.6 Creating Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778ed1a-9463-481e-96ed-d363e3717b38",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26678569-61a1-4b7e-b835-28ebbe2b4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3 ,5 ,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7f4af-9f7b-4b2d-81d6-9144a1f66a2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d1baf-36b1-47d4-a789-84d507af1338",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 100 for reproducibility purposes:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7b0e809-78b1-4b0a-8e81-68a171695186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1268,  1.3564, -0.0247],\n",
      "        [-0.8466,  0.0293, -0.5721],\n",
      "        [-1.2546,  0.0486,  0.2753],\n",
      "        [-2.1550, -0.7116,  0.0575],\n",
      "        [ 0.6263, -1.7736, -0.2205],\n",
      "        [ 2.7467, -1.0480,  1.1239]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(100)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea2ec6-3350-4d30-a62d-c4d124be958a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f53c0-9c19-493b-aa68-4a47ef8624e7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the embedding vector:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2faf9fe5-936d-4f19-afb1-951c810244df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1550, -0.7116,  0.0575]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7651b1-980d-4ee1-ac46-518468e54809",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row. In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e8af3-d0ce-47d0-9bda-30838d0435dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now apply that to all four input IDs we defined earlier (torch.tensor([2, 3, 5, 1])):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "960bc6c1-b8d4-4db9-81f4-f693baf1ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2546,  0.0486,  0.2753],\n",
      "        [-2.1550, -0.7116,  0.0575],\n",
      "        [ 2.7467, -1.0480,  1.1239],\n",
      "        [-0.8466,  0.0293, -0.5721]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8d95f-9bdf-4660-b090-770c28f289ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d605852-a1bd-4e33-90f1-a5d6931814c9",
   "metadata": {},
   "source": [
    "## 1.7 Postional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c11b8-78a8-40e1-8dc5-6786a6f76343",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Let's now consider more realistic embedding sizes and encode the input tokens into a 256-dimensional vector representation. \n",
    "\n",
    "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation. \n",
    "\n",
    "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8893f5f-d141-4cd4-968b-37ef0f469892",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787a36f-7f29-4877-84ac-7f06ceb21f32",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Using the `token_embedding_layer` above, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21c6e0-65ee-4ef9-935a-18c3c32f4b3e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's instantiate the data loader ( Data sampling with a sliding window), first:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "440da1f9-ee7f-42b6-87bd-75b6837461d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca8f4e-3deb-4669-b94b-7f513163824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs: \\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cae3b-ff38-4f04-b623-f8d50e49e4b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch consists of 8 text samples with 4 tokens each.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa38206-f93b-4478-9755-1cd9ee2536b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37eb6514-6059-4309-b2c4-951a57df6a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1c269-1c70-483c-8be1-8958eb9b3b3c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now embedded as a 256-dimensional vector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627df2db-f0d2-4b2f-8443-18432cfbfb4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same dimension as the token_embedding_layer:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8f49b8f-4462-42d7-9975-b86c31de6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b20c8e9-6867-4d32-8d4d-d525d6689357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4603,  1.8974,  2.3485,  ...,  1.5988,  1.2487, -1.3841],\n",
       "        [-1.2316, -0.0637, -1.1363,  ..., -0.4253,  0.9158, -1.4085],\n",
       "        [ 0.3789, -0.9401,  0.0262,  ...,  0.5510,  0.0613,  1.3368],\n",
       "        [ 0.3616, -0.5001, -0.5383,  ...,  0.4893, -1.1094, -0.3643]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ed7f3-10d1-4d71-904b-25874114f0b7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As shown in the preceding code example, the input to the `pos_embeddings_layer` is usually a placeholder vector torch.arange(context_length), which contains a sequence of numbers 0, 1, ..., up to the maximum input length âˆ’ 1. \n",
    "\n",
    "The context_length is a variable that represents the supported input size of the LLM. \n",
    "\n",
    "Here, we choose it similar to the maximum length of the input text. \n",
    "\n",
    "In practice, input text can be longer than the supported context length, in which case we have to truncate the text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b70bae3-2d90-49cd-b845-d6fa6f2c7c9b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4x256-dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in each of the 8 batches:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7d5b896-5ecc-4fc7-8cd7-eab82725f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings: torch.Size([8, 4, 256])\n",
      "Positional embeddings: torch.Size([4, 256])\n",
      "Input embeddings: torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embeddings:\", token_embeddings.shape)\n",
    "print(\"Positional embeddings:\", pos_embeddings.shape)\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Input embeddings:\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e56f57-1922-4e41-b4c2-52c3c748f9d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The input_embeddings we created are the embedded input examples that can now be processed by the main LLM modules\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
