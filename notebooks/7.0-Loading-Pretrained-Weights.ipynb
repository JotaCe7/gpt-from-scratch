{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab5cda6-0051-479c-bf81-a4ca12fb2256",
   "metadata": {},
   "source": [
    "# Chapter 7: Loading Pre-trained Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78631519-5be0-418f-a405-0aca30c7771d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In the previous chapters, we successfully built and trained a small GPT model from scratch. While this was a great learning exercise, real-world performance comes from models trained on massive, diverse datasets, which requires enormous computational resources.\n",
    "\n",
    "Fortunately, OpenAI released the weights for their trained GPT-2 models. In this chapter, we will load these professional, pre-trained weights into our own `GPTModel` architecture. This is the ultimate test of our implementation and will allow us to generate high-quality, coherent text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb14d47-fd4e-46e5-920f-4961c1056a58",
   "metadata": {},
   "source": [
    "## 7.1 Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d511c8-7aff-4b9f-84a0-815dcd2c38fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To load the original GPT-2 weights, which were saved in a TensorFlow checkpoint file, we need to install the `tensorflow` library. We will also use `tqdm` for a nice progress bar during the download.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fd6a17-52c0-4ee6-b6c8-8d19b132c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# !pip install tensorflow tqdm\n",
    "\n",
    "print(\"Installation complete\")\n",
    "\n",
    "# Standard imports and setup\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- Add Project Root to Python Path ---\n",
    "\n",
    "# Get the directory of the current notebook\n",
    "current_notebook_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project's root directory\n",
    "project_root = os.path.abspath(os.path.join(current_notebook_dir, '..'))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import from src package\n",
    "from src.config import GPT_CONFIG_124M\n",
    "from src.model import GPTModel\n",
    "from src.text_generation import generate\n",
    "from src.utils import text_to_token_ids, token_ids_to_text, download_file, load_gpt2_params_from_tf_ckpt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6820ac-64ec-4485-800b-e771aad773d6",
   "metadata": {},
   "source": [
    "## 7.2 Downloading the Pre-trained Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2efb4f-dcad-4224-9817-ab627d5e8901",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Previously, for educational purposes, we trained a small GPT-2 model using a limited dataset comprising a short-story book.\n",
    "\n",
    "This approach allowed us to focus on the fundamentals without the need for extensive time and computational resources.\n",
    "    \n",
    "Fortunately, OpenAI openly shared the weights of their GPT-2 models, thus eliminating the need to invest tens to hundreds of thousands of dollars in retraining the model on a large corpus ourselves.\n",
    "    \n",
    "In this chapter, we will load these weights into our GPTModel class and use the model for text generation. \n",
    "\n",
    "Here, weights refer to the weight parameters that are stored in the .weight attributes of PyTorch's Linear and Embedding layers, for example. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3640601-4cb6-4458-ac44-794f30107052",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "We'll define a utility function (`download_and_load_gpt2`) to download the necessary files for the GPT-2 124M model from the OpenAI repository if they don't already exist in our `models/gpt2/124M` directory.\n",
    "\n",
    "This function which will load the GPT-2 architecture **settings** (`settings`) and **weight parameters** (`params`).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0075502-130a-4aec-ae8b-a1cd53628f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path, backup_url)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c6231e-8d4e-4b18-909d-1d23bec6de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████████████████████| 77.0/77.0 [00:00<00:00, 76.9kiB/s]\n",
      "encoder.json: 100%|██████████████████████| 1.04M/1.04M [00:00<00:00, 1.21MiB/s]\n",
      "hparams.json: 100%|████████████████████████| 90.0/90.0 [00:00<00:00, 44.8kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████| 498M/498M [01:11<00:00, 6.96MiB/s]\n",
      "model.ckpt.index: 100%|██████████████████| 5.21k/5.21k [00:00<00:00, 5.20MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████| 471k/471k [00:00<00:00, 842kiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████| 456k/456k [00:00<00:00, 818kiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"../models/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44a08d-a18d-4b82-864d-1dde0ceea857",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "After the execution of the previous code has been completed, let's inspect the contents of settings and params:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030f33a9-affe-4bee-8f38-4d41121aac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca81ba-f848-4fc8-b1ce-23ae1a42582d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Both settings and params are Python dictionaries. The settings dictionary stores the LLM architecture settings similarly to our manually defined GPT_CONFIG_124M settings. \n",
    "\n",
    "The params dictionary contains the actual weight tensors. \n",
    "\n",
    "    \n",
    "Note that we only printed the dictionary keys because printing the weight contents would take up too much screen space\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60adbe3a-f638-4809-b119-c2cdd136e474",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We can inspect these weight tensors by printing the whole dictionary via print(params) or by selecting individual tensors via the respective dictionary keys, for example, the embedding layer weights:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45102e1-7510-4ea7-85ec-6fc1212eb3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c54748-e3fd-45a1-b109-f527147560f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "We downloaded and loaded the weights of the smallest GPT-2 model via the `download_and_load_gpt2(model_size=\"124M\", ...)` setting. However, note that OpenAI also shares the weights of larger models: \"355M\", \"774M\", and \"1558M\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50dda55-1c7e-492f-85b0-db7493313122",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Above, we loaded the **124M GPT-2** model weights into Python, however we still need to transfer them into our GPTModel instance.\n",
    "\n",
    "First, we initialize a new GPTModel instance.\n",
    "\n",
    "Note that the original GPT model initialized the **linear layers** for the query, key, and value matrices in the multi-head attention module with **bias vectors**, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting qkv_bias to True in our implementation, too.\n",
    "                                                                                                                                                                                                          \n",
    "Also, OpenAI used **bias vectors** in the **multi-head attention module's linear layers** to implement the query, key, and value matrix computations.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "**Bias vectors** are not commonly used in LLMs anymore as they don't improve the modeling performance and are thus unnecessary.\n",
    "</div>\n",
    "\n",
    "However, since we are working with pretrained weights, we need to match the settings for consistency and enable these bias vectors:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afdb2cec-39ef-4270-a3fe-16edf794c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "GPT_CONFIG_124M.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(GPT_CONFIG_124M)\n",
    "gpt.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873739b5-8665-468b-a218-8ee045cba885",
   "metadata": {},
   "source": [
    "## 7.3 Adapting the Weight Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771cebc1-0aec-432d-9b14-23b744191bad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "The parameter names in OpenAI's TensorFlow checkpoint (e.g., `attn/c_attn/w`) are different from the names in our PyTorch `GPTModel` (e.g., `attn.W_query.weight`).\n",
    "\n",
    "We must create a mapping function that carefully renames and reshapes the pre-trained weights to match our model's architecture precisely. This step is critical for a successful transfer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6128c-ae9c-4dcb-87e8-8fe32fd15da3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To assist with this mapping, we'll first define a small helper utility called `assign`. Its job is to ensure that a pre-trained weight tensor and our model's layer tensor have the exact same shape before assigning the weights. This acts as a valuable safety check.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e675f3-bb9e-40d7-bb1e-2b9cd7ac0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591afaee-b641-4a54-9119-07506750e4fd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The `load_weights_into_gpt` function bridges the gap between OpenAI's pre-trained parameters and our custom `GPTModel` architecture.\n",
    "\n",
    "It works by systematically assigning the pre-trained token and positional embedding weights to their corresponding layers. Then, it iterates through each transformer block, carefully mapping the various layer weights. A key part of this process is splitting the combined query, key, and value weights from the OpenAI checkpoint into the separate `W_query`, `W_key`, and `W_value` matrices in our attention module.\n",
    "\n",
    "Finally, it implements **weight tying** by assigning the token embedding weights to the model's final output head, ensuring a perfect one-to-one transfer of the learned parameters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624e33f5-a946-40b2-a9b6-96a304ae9563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].attn.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].attn.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].attn.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].attn.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].attn.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].attn.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].attn.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].attn.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bdf69a-51ca-4813-8c33-9339d5423aa4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Developing the load_weights_into_gpt function took a lot of guesswork since OpenAI used a slightly different naming convention from ours. \n",
    "\n",
    "However, the assign function would alert us if we try to match two tensors with different dimensions. \n",
    "\n",
    "Also, if we made a mistake in this function, we would notice this as the resulting GPT model would be unable to produce coherent text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879ac99-86d5-40e6-ab9b-268a0ba67f5d",
   "metadata": {},
   "source": [
    "## 7.4 Generating Text with Pre-trained Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3820695-d92f-4823-b78f-b17f2fca17a0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "With our mapping function ready, let's instantiate our `GPTModel`, load the adapted weights into it, and see the incredible difference in generation quality.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1fd085e-006d-4136-83b7-7dc8046dba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c20b3e7-9d71-49f5-b19b-6676cc55ed7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you through the world in different ways, not one based around your skills. Every job involves some other skill but to create a career\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c07162-78d5-460d-94f3-ab439f954743",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "**Success!**\n",
    "\n",
    "The model now generates perfectly **coherent and contextually relevant text**. This confirms that our from-scratch `GPTModel` architecture is implemented correctly and is compatible with the original GPT-2 design.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f47085-ba18-461c-b29e-6185c611af7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "## Chapter 7 Summary and Next Steps\n",
    "\n",
    "This concludes a major milestone in our project. We have successfully taken our from-scratch `GPTModel` architecture and loaded it with the professionally trained weights from OpenAI's original GPT-2 model.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    " \n",
    "**Milestone Reached: Pre-trained Model is Operational!**\n",
    "\n",
    "Throughout this chapter, we have:\n",
    "\n",
    "* Downloaded the official GPT-2 (124M) weights and configuration from OpenAI's repository.\n",
    "* Handled the original TensorFlow checkpoint format to extract the model's parameters.\n",
    "* Written a detailed mapping function to carefully adapt the names and shapes of the pre-trained weights to match our own `GPTModel` architecture.\n",
    "* Successfully loaded these weights into our model, replacing the random initial parameters.\n",
    "* Generated high-quality, coherent text, proving that our from-scratch implementation is correct and compatible with the original GPT-2 design.\n",
    "</div>\n",
    "\n",
    "### Where We Are Now\n",
    "We now have a powerful, pre-trained language model. It is no longer a toy model trained on a small text; it now has the general knowledge and language capabilities of the original GPT-2, learned from a massive web text corpus.\n",
    "\n",
    "### What's Next?\n",
    "Our model is now a powerful generalist, but it is not specialized for any particular task beyond text completion. The next step in the LLM lifecycle is to adapt this pre-trained model for a specific downstream task.\n",
    "\n",
    "In the next notebook, **Chapter 8: Finetuning for Text Classification**, we will take this powerful model and finetune it to become a spam classifier.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
