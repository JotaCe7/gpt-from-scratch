{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ae2d42-38a5-4d4d-8108-e62dc6b64d5c",
   "metadata": {},
   "source": [
    "# Chapter 2: Building a Production-Ready Attention  Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f6161-58b9-4caa-aa33-af524e6a222b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In Chapter 1, we built a simplified self-attention mechanism from first principles. While excellent for building intuition, that version was not flexible because each input vector had to act as its own query, key, and value.\n",
    "\n",
    "In this chapter, we will upgrade our mechanism to a \"production-ready\" version by introducing **trainable weight matrices**. This is the key step that allows the attention mechanism to *learn* the complex relationships in data, making it an incredibly powerful component of modern LLMs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cb52c-3145-464b-9dc0-685ef0a12ddc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Set up our environment with the necessary imports.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d03b768-454b-4e5d-b06c-ce36f0193cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb40f80-1e5d-4eff-9f17-ecf5912f889d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Introducing Trainable Weights (Wq, Wk, Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e14fd33-61c4-4496-bd98-e8e2d2a4e719",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For consistency, we will continue to use the same 6-token sample sentence and its 3-dimensional embedding vectors from the previous chapter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf736b9-3546-4db7-b2eb-c52d61e82feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample input sentence as embedding vectors\n",
    "inputs = torch.tensor(\n",
    "    [[ 0.8938,  0.9003,  0.8978], # Your\n",
    "     [ 0.7165,  0.3428,  0.2553], # journey\n",
    "     [ 0.1042,  0.5163,  0.3753], # starts\n",
    "     [ 0.0445,  0.3091,  0.9763], # with\n",
    "     [ 0.1554,  0.1614,  0.2700], # one\n",
    "     [ 0.8089,  0.9435,  0.5480]] # step\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169df531-df6b-4d29-bf87-713ede75891f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To make our attention mechanism more powerful and production-ready, we now introduce three dedicated, trainable **weight matrices**:\n",
    "\n",
    "* **`W_query` (Wq)**\n",
    "* **`W_key` (Wk)**\n",
    "* **`W_value` (Wv)**\n",
    "\n",
    "The purpose of these matrices is to **project** our input embeddings into three separate, specialized vectors. For each input token `x`, we will now calculate:\n",
    "\n",
    "1.  A **query vector `q`** (calculated as `x @ W_query`): This vector is optimized for asking the right \"question\" to find relevant keys.\n",
    "2.  A **key vector `k`** (calculated as `x @ W_key`): This vector is optimized to be effectively \"found\" by relevant queries.\n",
    "3.  A **value vector `v`** (calculated as `x @ W_value`): This vector contains the rich information that the token will contribute to the final output.\n",
    "\n",
    "Crucially, these matrices are **trainable parameters**. The model will learn the optimal values for these matrices during the training process, allowing it to master the complex art of understanding context in language.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89054285-0bc5-4de5-8c56-6f1d1a9b478d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "To see how this projection works in practice, let's focus on a single input token and define the dimensions for our weight matrices. For this hands-on example, we will:\n",
    "\n",
    "1.  Select the second input token (\"journey\") to be the **query** we analyze.\n",
    "2.  Get its embedding dimension from the input tensor (`d_in`).\n",
    "3.  Define a smaller output dimension (`d_out`) for the resulting query, key, and value vectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf43b948-0e42-465f-9969-aaf275468c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45c2bb-1067-4617-a7bd-7a0902f7813d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that in GPT-like models, the input and output dimensions are usually the same. \n",
    "\n",
    "But for illustration purposes,  we are using a smaller output dimension here simply to make the matrix operations easier to track visually.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728707a8-ab63-4517-8d0f-8f983d160591",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we initialize the three weight matrices Wq, Wk and Wv\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3d9c03-79ee-4df5-b09c-04095bb62411",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff2fcec-28da-4e32-a615-b501b67fb2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1117, 0.8158],\n",
      "        [0.2626, 0.4839],\n",
      "        [0.6765, 0.7539]])\n"
     ]
    }
   ],
   "source": [
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df787e3-c606-4b65-b104-f7b871ce734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2627, 0.0428],\n",
      "        [0.2080, 0.1180],\n",
      "        [0.1217, 0.7356]])\n"
     ]
    }
   ],
   "source": [
    "print(W_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70b8041-24e4-4bc4-8585-24c3d38056e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.7118, 0.7876],\n",
      "        [0.4183, 0.9014],\n",
      "        [0.9969, 0.7565]])\n"
     ]
    }
   ],
   "source": [
    "print(W_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04512cad-fe73-442a-a741-589ec69748d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we are setting requires_grad=False to reduce clutter in the outputs for illustration purposes. \n",
    "\n",
    "If we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb771fd3-7c76-4594-bf68-bf584483c4f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we compute the query, key, and value vectors as shown earlier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6efd5a3-7a42-4a03-932d-2605e12218a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3427, 0.9429])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18823c-886c-464c-9f22-ca488779e5d4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see based on the output for the query, this results in a 2-dimensional vector. \n",
    "\n",
    "This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31d5c8-608b-4a08-8ed4-61c4e8a4864f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Even though our temporary goal is to only compute the one context vector z(2),  we still require the key and value vectors for all input elements. \n",
    "\n",
    "This is because they are involved in computing the attention weights with respect to the query q(2)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46851a7-1709-4b99-aefc-7ca3697c1c48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can obtain all keys and values via matrix multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "785228e7-b21d-44d2-81b5-24787219e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cca6b1-1740-4176-886d-ef8f4f1af0dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D onto a 2D embedding space:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12abee21-ff86-415e-88ab-6004b2b4dd64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.2 Scaling Attention Scores to create Attention Weights and Context Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909b52d-ade7-4409-b433-27c181c1871b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "First, let's compute the attention score ω22\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286ada9f-937a-4c88-b74b-bc64ab13262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3438)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19621f6-06e4-4ff0-a882-8c96f926362a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Again, we can generalize this computation to all attention scores via matrix multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aaf99ff-2f4c-4a8c-b872-a1b635a24209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9411, 0.3438, 0.3838, 0.7801, 0.2483, 0.6807])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b158fa8-f165-410f-8f8d-bd17dd647a0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We compute the attention weights by scaling the attention scores and using the softmax function we used earlier. \n",
    "\n",
    "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df6f8bdf-8e4c-488b-a1a0-6e415e7fe5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the second input: tensor([0.2143, 0.1405, 0.1445, 0.1912, 0.1313, 0.1782])\n",
      "Embedding dimension for the keys: 2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(\"Attention weights for the second input:\", attn_weights_2)\n",
    "print(\"Embedding dimension for the keys:\", d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06d402-7a69-4d5a-9dba-f2386841d911",
   "metadata": {},
   "source": [
    "### Why divide by the square root of the embedding dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e6431-a7a8-4665-b866-6c921f116f4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Reason 1: For stability in learning</b>\n",
    "\n",
    "The softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives almost all the probability mass, and the rest receive very little.ery sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab7a81a4-c5ca-4e78-a9a2-4ff4b46774e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "Softmax after scaling by 8: tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "# Apply softmax without scaling\n",
    "softmax_result = torch.softmax(tensor, dim=-1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# Multiply the tensor by 8 and then apply softmax\n",
    "scaled_tensor = 8 * tensor\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(\"Softmax after scaling by 8:\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fb926-6878-4997-8ef3-3455a7dbe517",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become too large (like multiplying by 8 in this example), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae8135-0064-4b3c-a385-4a151fd40fb2",
   "metadata": {},
   "source": [
    "### But, why by the square root?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482902e-015e-4aed-9967-ad673df033a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>Reason 2: To make the variance of the dot product stable</b>\n",
    "\n",
    "The dot product of  Q and K increases the variance because multiplying two random numbers increases the variance.\n",
    "\n",
    "The increase in variance grows with the dimension. \n",
    "\n",
    "Dividing by sqrt (dimension) keeps the variance close to 1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48d4b220-4684-43b9-9b10-bc56d2e182fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 4.4993367243501625\n",
      "Variance after scaling (dim=5): 0.8998673448700324\n",
      "Variance before scaling (dim=20): 21.491231475308616\n",
      "Variance after scaling (dim=20): 1.0745615737654306\n"
     ]
    }
   ],
   "source": [
    "# Function to compute variance before and after scaling\n",
    "def compute_variances(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute the products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        # Compuute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "\n",
    "    # Calculate the variance of the dot produucts\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "# For dimension 5:\n",
    "variance_before_scaling_5, variance_after_scaling_5 = compute_variances(dim=5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_scaling_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_scaling_5}\")\n",
    "\n",
    "# For dimension 20:\n",
    "variance_before_scaling_20, variance_after_scaling_20 = compute_variances(dim=20)\n",
    "print(f\"Variance before scaling (dim=20): {variance_before_scaling_20}\")\n",
    "print(f\"Variance after scaling (dim=20): {variance_after_scaling_20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f379e-9276-4a91-a410-59033474cd06",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We now compute the context vector as a weighted sum over the value vectors. \n",
    "\n",
    "Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. \n",
    "\n",
    "We can use matrix multiplication to obtain the output in one step:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a37cd3-d95c-4e70-b3c8-e4a3bf9222f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for the second input: tensor([1.1783, 1.3425])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context vector for the second input:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aab0da-3816-4988-a9ab-49f98ec0f802",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "So far, we only computed a single context vector, z(2). \n",
    "\n",
    "In the next section, we will generalize the code to compute all context vectors in the input sequence, z(1)to z (T)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de9318-6aa4-4ea0-b9ae-735197822578",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.3 Implementing a Compact Self Attention Python Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ec0b2-69ab-4c49-a64a-9e6ddebdae03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In the previous sections, we have gone through a lot of steps to compute the self-attention outputs. \n",
    "\n",
    "This was mainly done for illustration purposes so we could go through one step at a time. \n",
    "\n",
    "In practice, with the LLM implementation in the next chapter in mind, it is helpful to organize this code into a Python class as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f807d67f-60bd-441a-b7bd-a2f45638e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T #omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / d_out**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada6055-933e-457e-a029-fa57d05264e5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models, which provides necessary functionalities for model layer creation and management.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebeac9f-1dd0-4994-a3b6-5b9ac726ae3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
    "\n",
    "Finally, we create a context vector by weighting the values with these normalized attention scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6c92321-c8d4-41e2-af64-53dc6a17ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2705, 1.4457],\n",
      "        [1.1783, 1.3425],\n",
      "        [1.1593, 1.3236],\n",
      "        [1.1985, 1.3688],\n",
      "        [1.1366, 1.2980],\n",
      "        [1.2373, 1.4083]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555d9a8-96dd-44ce-9fea-d03323a8c3e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Since inputs contains six embedding vectors, we get a matrix storing the six context vectors, as shown in the above result. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d5266-cc0f-4df2-ad67-25ec1fba2f08",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As a quick check, notice how the second row ([1.1783, 1.3425]) matches the contents of context_vec_2 in the previous section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c4f61-1cd6-4262-a835-8e8e0770614b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix multiplication when the bias units are disabled. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5857c5-fc80-411f-aa6d-afca8d17f566",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Additionally, a significant advantage of using nn.Linear instead of manually implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight initialization scheme, contributing to more stable and effective model training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd25300c-ed95-45a2-965a-e16ba93285ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T #omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / d_out**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b561ca-c97c-496a-8a1e-22ed4e0fb542",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c39dcf6c-1009-4735-aa8c-c96c2bf867b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2303, 0.6761],\n",
      "        [0.2381, 0.6888],\n",
      "        [0.2268, 0.6639],\n",
      "        [0.2287, 0.6709],\n",
      "        [0.2335, 0.6777],\n",
      "        [0.2288, 0.6711]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc647f-587f-4da7-b63c-3f83b40cdd76",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices since nn.Linear uses a more sophisticated weight initialization scheme.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5b738-828d-40b8-9cd4-ffdfbadd33e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.4 Causal Attention: Masking for Autoregressive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7fb2e-b9a2-461f-b2a8-73169aa4500f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In a language model that generates text one token at a time (an <b>autoregressive</b> model), there's a strict rule: to predict the next token, the model is only allowed to see the tokens that came before it. It cannot \"see into the future.\"\n",
    "\n",
    "The self-attention mechanism we've built so far violates this rule! Currently, every token can \"see\" and gather context from all other tokens in the sequence, including those that appear later. This is like giving the model the answers to the test during training.\n",
    "\n",
    "To fix this, we must implement <b>causal attention</b>. The goal is to \"mask\" or hide these connections to future tokens.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809fc17-940d-44f6-9c62-1f8139334dc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "First, let's recalculate the full attention weights matrix to see the problem clearly. Notice in the output below how every token attends to every other token (all weights are non-zero). For example, \"journey\" (the second row) is currently paying attention to \"starts\", \"with\", \"one\", and \"step\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49e7e0b4-408a-4af9-bd51-a6d206dac2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample input sentence as embedding vectors\n",
    "inputs = torch.tensor(\n",
    "    [[ 0.8938,  0.9003,  0.8978], # Your\n",
    "     [ 0.7165,  0.3428,  0.2553], # journey\n",
    "     [ 0.1042,  0.5163,  0.3753], # starts\n",
    "     [ 0.0445,  0.3091,  0.9763], # with\n",
    "     [ 0.1554,  0.1614,  0.2700], # one\n",
    "     [ 0.8089,  0.9435,  0.5480]] # step\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd404c0-c36a-4bb4-8a11-36218224a45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1694, 0.1562, 0.1660, 0.1839, 0.1634, 0.1611],\n",
      "        [0.1763, 0.1609, 0.1632, 0.1700, 0.1584, 0.1712],\n",
      "        [0.1581, 0.1642, 0.1701, 0.1770, 0.1737, 0.1568],\n",
      "        [0.1646, 0.1593, 0.1677, 0.1814, 0.1677, 0.1592],\n",
      "        [0.1676, 0.1639, 0.1665, 0.1709, 0.1657, 0.1654],\n",
      "        [0.1647, 0.1595, 0.1677, 0.1810, 0.1677, 0.1594]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Reusing the sa_v2 object from the previous section\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c4911-21ec-4264-9bf1-7306aa15a63e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As you can see, the attention matrix is fully populated. To enforce causality, we will create a <b>look-ahead mask</b>. This is an upper-triangular matrix that will hide all the positions that correspond to future tokens.\n",
    "\n",
    "We will apply this mask to the attention scores *before* the softmax step by replacing the scores for future positions with negative infinity (`-inf`). When softmax is applied, $e^{-\\infty}$ becomes zero, ensuring no attention is paid to those future tokens.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eb451ad-9334-4517-9b26-37e6ea9842a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_weights.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb70882-bed5-4fb5-a7d4-78ea2951ab72",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, we can multiply this mask with the attention weights to zero out the values above the diagonal:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2349076-9ed0-427e-912b-da3eaf1278dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1694, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1763, 0.1609, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1581, 0.1642, 0.1701, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1646, 0.1593, 0.1677, 0.1814, 0.0000, 0.0000],\n",
      "        [0.1676, 0.1639, 0.1665, 0.1709, 0.1657, 0.0000],\n",
      "        [0.1647, 0.1595, 0.1677, 0.1810, 0.1677, 0.1594]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e707e73-35c7-4477-ac8f-e0eae2ce62ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, the elements above the diagonal are successfully zeroed out\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34989b5c-c07c-4f24-91c7-38dc132c68e4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The third step is to renormalize the attention weights to sum up to 1 again in each row. \n",
    "\n",
    "We can achieve this by dividing each element in each row by the sum in each row:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d92b254-0386-4724-834a-4f7cf6ebe5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5229, 0.4771, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3212, 0.3334, 0.3454, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2367, 0.2492, 0.2695, 0.0000, 0.0000],\n",
      "        [0.2008, 0.1964, 0.1995, 0.2048, 0.1986, 0.0000],\n",
      "        [0.1647, 0.1595, 0.1677, 0.1810, 0.1677, 0.1594]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8799626e-e210-45d6-9e04-d7d0e8a0e3dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "The result is an attention weight matrix where the attention weights above the diagonal are zeroed out and where the rows sum to 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa307c-3cfd-44e6-8fdc-a491b18b0ab3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "While we could be technically done with implementing causal attention at this point, we can take advantage of a mathematical property of the softmax function. \n",
    "\n",
    "We can implement the computation of the masked attention weights more efficiently in fewer steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3aae9-994d-4f2c-9fd4-793470548fc4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "The softmax function converts its inputs into a probability distribution. \n",
    "\n",
    "When negative infinity values (-∞) are present in a row, the softmax function treats them as zero probability. \n",
    "\n",
    "(Mathematically, this is because e^-∞ approaches 0.)\n",
    "\n",
    "\n",
    "We can implement this more efficient masking \"trick\" by creating a mask with 1's above the diagonal and then replacing these 1's with negative infinity (-inf) values:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23ce2e42-0bfc-4710-9b30-9f50e4ec3853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0922, -0.0228,  0.0634,  0.2085,  0.0415,  0.0216],\n",
      "        [ 0.2010,  0.0713,  0.0916,  0.1489,  0.0496,  0.1594],\n",
      "        [-0.1554, -0.1025, -0.0527,  0.0042, -0.0225, -0.1672],\n",
      "        [-0.0104, -0.0570,  0.0158,  0.1269,  0.0154, -0.0578],\n",
      "        [ 0.0275, -0.0039,  0.0178,  0.0549,  0.0114,  0.0091],\n",
      "        [-0.0108, -0.0557,  0.0150,  0.1227,  0.0148, -0.0567]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ed0c67a-c904-47c8-87f7-b103399b5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0922,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.2010,  0.0713,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.1554, -0.1025, -0.0527,    -inf,    -inf,    -inf],\n",
      "        [-0.0104, -0.0570,  0.0158,  0.1269,    -inf,    -inf],\n",
      "        [ 0.0275, -0.0039,  0.0178,  0.0549,  0.0114,    -inf],\n",
      "        [-0.0108, -0.0557,  0.0150,  0.1227,  0.0148, -0.0567]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "attn_scores_masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(attn_scores_masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00afc21-ec5b-41f1-a576-2d165d0e1ab5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, all we need to do is apply the softmax function along the last dimension to these masked results to normalize the scores for each query, and we are done.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60f0a9da-c06d-492a-9a46-1cbb71f9af06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5229, 0.4771, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3212, 0.3334, 0.3454, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2446, 0.2367, 0.2492, 0.2695, 0.0000, 0.0000],\n",
      "        [0.2008, 0.1964, 0.1995, 0.2048, 0.1986, 0.0000],\n",
      "        [0.1647, 0.1595, 0.1677, 0.1810, 0.1677, 0.1594]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores_masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6012de-7c27-4001-8a2c-12bc17990615",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see based on the output, the values in each row sum to 1, and no further normalization is necessary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f2b1c-e0ae-45fc-a4c3-4bd395e121b9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Masking in Transformers sets scores for future tokens to a large negative value, making their influence in the softmax calculation effectively zero. \n",
    "\n",
    "The softmax function then recalculates attention weights only among the unmasked tokens. \n",
    "\n",
    "This process ensures no information leakage from masked tokens, focusing the model solely on the intended data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a120d2-3ed4-4ed1-a80d-fd7c58dcf651",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "We could now use the modified attention weights to compute the context vectors via context_vec = attn_weights @ values.\n",
    "\n",
    "However, in the next section, we first cover another minor tweak to the causal attention mechanism that is useful for reducing overfitting when training LLMs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dc029d-9179-4759-a336-6c1836e43863",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.5 Masking Attention Weights With Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c829cbf-565f-4e18-8722-b1613c139f77",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "A common challenge when training large neural networks is <b>overfitting</b>. This happens when a model learns the training data <i>too well</i> —including its noise and specific quirks— and then fails to generalize to new, unseen examples. To combat overfitting, we use techniques called <b>regularization</b>.\n",
    "\n",
    "<b>Dropout</b> is one of the most effective and widely used regularization methods in deep learning.\n",
    "\n",
    "#### How Dropout Works\n",
    "During each training step, dropout randomly sets a fraction of neuron activations or weights to zero. This forces the network to learn more robust and redundant representations because it cannot become too reliant on any single connection or feature, as it might be \"dropped out\" at any moment.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Analogy: A Team of Experts</b><br>\n",
    "  Imagine a team of experts solving a problem. If the same experts are always available, they might start relying too heavily on each other's specific knowledge. Dropout is like telling some experts to randomly take a break for each task. The remaining team members are forced to learn how to cover for them and develop a wider range of skills, making the whole team more robust.\n",
    "</div>\n",
    "\n",
    "By applying dropout to the attention weights, we prevent the attention mechanism from \"overfitting\" on specific token-to-token relationships it sees in the training data. It encourages the model to learn more diverse ways of gathering context.\n",
    "\n",
    "To see how it works in practice, let's first apply dropout to a simple matrix of ones. We'll use a high dropout rate of 50% (`p=0.5`) for this demonstration to make its effect obvious. Note that dropout is **only active during training** and is automatically disabled during evaluation (`model.eval()`).\n",
    "\n",
    "When we train the GPT model in later chapters, we will use a lower dropout rate, such as 0.1 or 0.2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d39c382-cec1-4a1f-a8e0-609223241bf2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In the following code, we apply PyTorch's dropout implementation first to a 6×6 tensor consisting of ones for illustration purposes:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7de3b80-fef7-4f86-a1d7-32e6a2e2ceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "example = torch.ones(6, 6)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa48354c-d55f-4d56-92b1-84f92f5d8b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 2., 2., 2., 0., 0.],\n",
      "        [0., 2., 0., 0., 2., 2.],\n",
      "        [0., 0., 0., 2., 2., 2.],\n",
      "        [0., 2., 2., 2., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [2., 0., 2., 2., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "dropout = torch.nn.Dropout(0.5) #A\n",
    "example = torch.ones(6, 6) #B\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8a1b2-3a2e-4f2d-ae0c-2f2ec7e0d5ca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the elements in the matrix are randomly set to zero. \n",
    "\n",
    "To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of\n",
    "1/(1-0.5) =2. \n",
    "\n",
    "This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both the training and inference phases.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef16196-689b-493a-9fda-e9611becaba2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Now, let's apply dropout to the attention weight matrix itself:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "691db547-aa4a-4fae-9a10-e52163aaa144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9542, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4733, 0.4984, 0.5391, 0.0000, 0.0000],\n",
      "        [0.4016, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3293, 0.0000, 0.3354, 0.3620, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70e76c-5f31-4f6b-b333-6b69d054fc5c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see above, the resulting attention weight matrix now has additional elements zeroed out and the remaining ones rescaled.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2e123-5f21-479b-9490-d0cedfd9603a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Having gained an understanding of causal attention and dropout masking, we will develop a concise Python class in the following section. \n",
    "    \n",
    "This class is designed to facilitate the efficient application of these two techniques.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84782b3-e537-4b3c-8b7c-fe0a2d9a430a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.6 Implementing a Compact Causal Attention Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317cb8a-4cdb-46ce-ac0d-e78483a7e2e3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "In this section, we will now incorporate the causal attention and dropout modifications into the SelfAttention Python class we developed in section 2.3. \n",
    "\n",
    "This class will then serve as a template for developing multi-head attention in the upcoming section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73eb70e-0c00-48f0-8dab-83e4d8becb34",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Before we begin, one more thing is to ensure that the code can handle batches consisting of more than one input. \n",
    "\n",
    "This will ensure that the CausalAttention class supports the batch outputs produced by the data loader we implemented earlier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ce0aea-cec1-489f-bd8f-ab2a34246b97",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a46b289d-1d23-4cef-8f60-1df539e71657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Our sample input sentence as embedding vectors\n",
    "inputs = torch.tensor(\n",
    "    [[ 0.8938,  0.9003,  0.8978], # Your\n",
    "     [ 0.7165,  0.3428,  0.2553], # journey\n",
    "     [ 0.1042,  0.5163,  0.3753], # starts\n",
    "     [ 0.0445,  0.3091,  0.9763], # with\n",
    "     [ 0.1554,  0.1614,  0.2700], # one\n",
    "     [ 0.8089,  0.9435,  0.5480]] # step\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e8892-6e0d-454c-b13d-146fbe28022d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "This results in a 3D tensor consisting of 2 input texts with 6 tokens each, where each token is a 3-dimensional embedding vector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240d463-ed0c-449d-b7b0-1438f6ea070f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The following CausalAttention class is similar to the SelfAttention class we implemented earlier, except that we now added the dropout and causal mask components as highlighted in the following code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66047208-3ce4-4038-9dc8-476ae634e276",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<li><b>Step 1:</b> Compared to the previous SelfAttention_v1 class, we added a dropout layer.</li>\n",
    "    \n",
    "<li><b>Step 2:</b> The register_buffer call is also a new addition (more information is provided in the following text).</li>\n",
    "\n",
    "<li><b>Step 3:</b>  We transpose dimensions 1 and 2, keeping the batch dimension at the first position (0).</li>\n",
    "\n",
    "<li><b>Step 4:</b> In PyTorch, operations with a trailing underscore are performed in-place, avoiding unnecessary memory copies</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6b196fe-d322-4cf4-9a4f-f0bd1258e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in, d_out, context_length, dropout_rate, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.register_buffer('causal_mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_im = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.causal_mask.bool()[:num_tokens,:num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim = 1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414d920-3ed1-4c00-a461-bd51fc6d3312",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The use of register_buffer in PyTorch is not strictly necessary for all use cases but offers several advantages here. \n",
    "\n",
    "For instance, when we use the CausalAttention class in our LLM, buffers are automatically moved to the appropriate device (CPU or GPU) along with our model, which will be relevant when training the LLM in future chapters. \n",
    "\n",
    "This means we don't need to manually ensure these tensors are on the same device as your model parameters, avoiding device mismatch errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62932e-cb21-40f1-8b06-54048825708d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can use the CausalAttention class as follows, similar to SelfAttention previously:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc7ca708-ea3a-4fe3-b29e-cba0e1d768f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "350ad4ee-8c70-40d2-8ed1-7d083afc6673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0755, 0.2087],\n",
      "         [0.1384, 0.3551],\n",
      "         [0.1526, 0.3997],\n",
      "         [0.1702, 0.5931],\n",
      "         [0.2069, 0.7170],\n",
      "         [0.6627, 1.7936]],\n",
      "\n",
      "        [[0.0755, 0.2087],\n",
      "         [0.1384, 0.3551],\n",
      "         [0.1526, 0.3997],\n",
      "         [0.1702, 0.5931],\n",
      "         [0.2069, 0.7170],\n",
      "         [0.6627, 1.7936]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4362c-25d2-4f88-9201-e5363f6d894b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can see, the output is a `(batch_size, num_tokens, d_out)` tensor. Each of the original input tokens has now been transformed into a new context-aware embedding vector of size `d_out`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dcfc33-a242-41ee-9a34-e1f498b013da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "  <b>An Interesting Observation: Deterministic Outputs</b><br>\n",
    "  You might notice that the output context vectors for both items in our batch are identical. This is the expected behavior if the two input sentences in the <code>batch</code> tensor were also identical.\n",
    "  <br><br>\n",
    "  This serves as a good sanity check, confirming that our attention module is <b>deterministic</b>. With a fixed set of weights (which we have, since the model is untrained), the same input will always produce the exact same output. The transformation is complex, but it is not random.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2a316-f8cb-4d32-840d-94598ea5373c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Now that we have a complete single-head causal attention module, the next logical step is to scale this concept up. We will now build a `MultiHeadAttention` module, which runs several of these attention mechanisms in parallel to capture an even richer variety of relationships within the text.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ee64b-9c0a-4f00-8e2d-e87d5880cc9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.7 Extending Single-Head Attention to Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5e57d-820e-4a65-89e4-d886bcfbc629",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "So far, we have built a single self-attention mechanism, also known as an \"attention head.\" While powerful, a single head can be a bottleneck; it might learn to focus on one type of relationship (for example, the relationship between a subject and its verb), but struggle to capture other kinds of linguistic patterns simultaneously.\n",
    "\n",
    "To make the model more powerful, the original transformer architecture introduced **Multi-Head Attention**.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Analogy: A Committee of Experts</b><br>\n",
    "  Instead of having one attention mechanism (one 'expert') trying to understand all the different relationships in a sentence, Multi-Head Attention creates a <b>committee of experts</b> (multiple attention 'heads'). \n",
    "\n",
    "  Each head works completely independently and in parallel. During training, each head can learn to specialize in detecting a different type of feature. For example:\n",
    "  <ul>\n",
    "    <li>One head might learn to track syntactic dependencies.</li>\n",
    "    <li>Another might learn to track semantic similarity.</li>\n",
    "    <li>A third might learn to track the proximity of words.</li>\n",
    "  </ul>\n",
    "  The outputs of all these specialist heads are then concatenated to form a final, rich representation of the token's context.\n",
    "</div>\n",
    "\n",
    "In our example below, we will create a `MultiHeadAttention` module with **two heads**. Each head will produce a **2-dimensional** context vector (`d_out=2`). After concatenating the outputs of both heads, we will get a final **4-dimensional** context vector for each token (`2 heads * 2 dimensions/head = 4 dimensions`).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbd6ea-134e-4a09-a3c7-ddf8bf67de84",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "In code, we can achieve this by implementing a simple MultiHeadAttentionWrapper class that stacks multiple instances of our previously implemented CausalAttention module:    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b7c05ef-56f3-43cc-9587-b12cf00b3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6804a-d92e-4c4b-bdcc-9378ed6a9ef0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "To illustrate further with a concrete example, we can use the MultiHeadAttentionWrapper class similar to the CausalAttention class before:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d4aa5dc-06a3-41e3-b5e8-06ecb2ca5e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Our sample input sentence as embedding vectors\n",
    "inputs = torch.tensor(\n",
    "    [[ 0.8938,  0.9003,  0.8978], # Your\n",
    "     [ 0.7165,  0.3428,  0.2553], # journey\n",
    "     [ 0.1042,  0.5163,  0.3753], # starts\n",
    "     [ 0.0445,  0.3091,  0.9763], # with\n",
    "     [ 0.1554,  0.1614,  0.2700], # one\n",
    "     [ 0.8089,  0.9435,  0.5480]] # step\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "695e2357-582d-434c-9769-07e2cea31625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0755,  0.2087,  0.0093, -0.1200],\n",
      "         [ 0.1384,  0.3551,  0.0133, -0.2169],\n",
      "         [ 0.1526,  0.3997,  0.0085, -0.3357],\n",
      "         [ 0.1702,  0.5931,  0.0877, -0.4786],\n",
      "         [ 0.2069,  0.7170,  0.1177, -0.6183],\n",
      "         [ 0.6627,  1.7936,  0.0402, -1.3122]],\n",
      "\n",
      "        [[ 0.0755,  0.2087,  0.0093, -0.1200],\n",
      "         [ 0.1384,  0.3551,  0.0133, -0.2169],\n",
      "         [ 0.1526,  0.3997,  0.0085, -0.3357],\n",
      "         [ 0.1702,  0.5931,  0.0877, -0.4786],\n",
      "         [ 0.2069,  0.7170,  0.1177, -0.6183],\n",
      "         [ 0.6627,  1.7936,  0.0402, -1.3122]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "context_length =  batch.shape[1] # This is the number of tokens = 6\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e4d2b-14aa-466c-b46d-69e831d9144c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The first dimension of the resulting context_vecs tensor is 2 since we have two input texts (the input texts are duplicated, which is why the context vectors are exactly the same for those). \n",
    "\n",
    "The second dimension refers to the 6 tokens in each input. The third dimension refers to the 4-dimensional embedding of each token.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182ec00-7b8b-43c6-b5a0-41f6963451fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In this section, we implemented a MultiHeadAttentionWrapper that combined multiple single-head attention modules. \n",
    "\n",
    "However, note that these are processed sequentially via [head(x) for head in self.heads] in the forward method. \n",
    "\n",
    "We can improve this implementation by processing the heads in parallel. \n",
    "\n",
    "One way to achieve this is by computing the outputs for all attention heads simultaneously via matrix multiplication, as we will explore in the next section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a71bc0-8803-4bda-b4a5-be997e189a52",
   "metadata": {},
   "source": [
    "## 2.8 Implementing Multi-Head Attention With Weight Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6d5fb-577d-4b03-8018-d68633798963",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "Instead of maintaining two separate classes, MultiHeadAttentionWrapper and CausalAttention, we can combine both of these concepts into a single MultiHeadAttention class. \n",
    "\n",
    "Also, in addition to just merging the MultiHeadAttentionWrapper with the CausalAttention code, we will make some other modifications to implement multi-head attention more efficiently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa23c59-0f37-4b4e-957f-5b988d76180d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In the MultiHeadAttentionWrapper, multiple heads are implemented by creating a list of CausalAttention objects (self.heads), each representing a separate attention head.\n",
    "\n",
    "The CausalAttention class independently performs the attention mechanism, and the results from each head are concatenated.\n",
    "\n",
    "In contrast, the following MultiHeadAttention class integrates the multi-head functionality within a single class. \n",
    "\n",
    "It splits the input into multiple heads by reshaping the projected query, key, and value tensors and then combines the results from these heads after computing attention.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a2beb-20a5-4ec4-b02b-df19a5c7d3ad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let's take a look at the MultiHeadAttention class before we discuss it further:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c1e252e-2988-4bdd-918b-db7b8e551fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired ouput dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('causal_mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Project inputs into Q, K, V\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Reshape for multi-head attention by splitting the d_out dimension\n",
    "        # (batch_size, num_tokens, d_out) -> (batch_size, num_tokens, num_heads, head_dim)\n",
    "        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose to bring the 'num_heads' dimension forward for batch matrix multiplication\n",
    "        # (batch_size, num_tokens, num_heads, head_dim) -> (batch_size, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention \n",
    "        attn_scores = queries @ keys.transpose(-2, -1) # Dot product for each head\n",
    "\n",
    "        # Apply the causal mask\n",
    "        causal_mask_bool = self.causal_mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill(causal_mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Apply attention weights to values and reverse the transpose\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads back into a single tensor: self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "\n",
    "        ## Apply the final linear projection\n",
    "        context_vec = self.out_proj(context_vec) # Optional projection\n",
    "\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297cb6f-7119-4e4f-ae7a-76de24a29ddf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Instead of creating separate `CausalAttention` objects for each head (which is inefficient), a more common and optimized approach is to use one large linear layer for all queries, keys, and values, and then **\"split\"** the resulting matrix into multiple heads for parallel processing. This is achieved through clever tensor reshaping.\n",
    "\n",
    "The `forward` pass of this new `MultiHeadAttention` class follows these key steps:\n",
    "\n",
    "1.  **Project into Q, K, V:** The input `x` is first passed through the three large `W_query`, `W_key`, and `W_value` linear layers to get the initial `queries`, `keys`, and `values` tensors of shape `(batch_size, num_tokens, d_out)`.\n",
    "\n",
    "2.  **Split into Heads:** We then reshape each of these tensors to add the `num_heads` dimension. The `.view()` method unrolls the `d_out` dimension into `num_heads` and `head_dim`. The new shape is `(batch_size, num_tokens, num_heads, head_dim)`.\n",
    "\n",
    "3.  **Transpose for Calculation:** To perform matrix multiplication across all heads at once, we need the `num_heads` dimension to be second. We use `.transpose(1, 2)` to swap the `num_tokens` and `num_heads` dimensions, resulting in a shape of `(batch_size, num_heads, num_tokens, head_dim)`.\n",
    "\n",
    "4.  **Compute Attention:** With the tensors in the correct shape, we can now compute the scaled dot-product attention just like before. This calculation happens independently for all heads in parallel.\n",
    "\n",
    "5.  **Combine Heads:** Finally, we reverse the process. We transpose the dimensions back and then use `.view()` to flatten the `num_heads` and `head_dim` dimensions back into the single `d_out` dimension. This concatenated result is then passed through a final linear projection layer (`out_proj`).\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
