{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ae2d42-38a5-4d4d-8108-e62dc6b64d5c",
   "metadata": {},
   "source": [
    "# Chapter 2: Building a Production-Ready Attention  Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cb52c-3145-464b-9dc0-685ef0a12ddc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Set up our environment with the necessary imports.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d03b768-454b-4e5d-b06c-ce36f0193cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb40f80-1e5d-4eff-9f17-ecf5912f889d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2.1 Introducing Trainable Weights (Wq, Wk, Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e14fd33-61c4-4496-bd98-e8e2d2a4e719",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Let'use the sample sentence we uswed in the previous chapter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf736b9-3546-4db7-b2eb-c52d61e82feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our sample input sentence as embedding vectors\n",
    "inputs = torch.tensor(\n",
    "    [[ 0.8938,  0.9003,  0.8978], # Your\n",
    "     [ 0.7165,  0.3428,  0.2553], # journey\n",
    "     [ 0.1042,  0.5163,  0.3753], # starts\n",
    "     [ 0.0445,  0.3091,  0.9763], # with\n",
    "     [ 0.1554,  0.1614,  0.2700], # one\n",
    "     [ 0.8089,  0.9435,  0.5480]] # step\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169df531-df6b-4d29-bf87-713ede75891f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "To make our attention mechanism more powerful and production-ready, we now introduce three dedicated, trainable **weight matrices**:\n",
    "\n",
    "* **`W_query` (Wq)**\n",
    "* **`W_key` (Wk)**\n",
    "* **`W_value` (Wv)**\n",
    "\n",
    "The purpose of these matrices is to **project** our input embeddings into three separate, specialized vectors. For each input token `x`, we will now calculate:\n",
    "\n",
    "1.  A **query vector `q`** (calculated as `x @ W_query`): This vector is optimized for asking the right \"question\" to find relevant keys.\n",
    "2.  A **key vector `k`** (calculated as `x @ W_key`): This vector is optimized to be effectively \"found\" by relevant queries.\n",
    "3.  A **value vector `v`** (calculated as `x @ W_value`): This vector contains the rich information that the token will contribute to the final output.\n",
    "\n",
    "Crucially, these matrices are **trainable parameters**. The model will learn the optimal values for these matrices during the training process, allowing it to master the complex art of understanding context in language.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89054285-0bc5-4de5-8c56-6f1d1a9b478d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "To see how this projection works in practice, let's focus on a single input token and define the dimensions for our weight matrices. For this hands-on example, we will:\n",
    "\n",
    "1.  Select the second input token (\"journey\") to be the **query** we analyze.\n",
    "2.  Get its embedding dimension from the input tensor (`d_in`).\n",
    "3.  Define a smaller output dimension (`d_out`) for the resulting query, key, and value vectors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf43b948-0e42-465f-9969-aaf275468c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45c2bb-1067-4617-a7bd-7a0902f7813d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that in GPT-like models, the input and output dimensions are usually the same. \n",
    "\n",
    "But for illustration purposes,  we are using a smaller output dimension here simply to make the matrix operations easier to track visually.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728707a8-ab63-4517-8d0f-8f983d160591",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we initialize the three weight matrices Wq, Wk and Wv\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af3d9c03-79ee-4df5-b09c-04095bb62411",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ff2fcec-28da-4e32-a615-b501b67fb2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.1117, 0.8158],\n",
      "        [0.2626, 0.4839],\n",
      "        [0.6765, 0.7539]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5df787e3-c606-4b65-b104-f7b871ce734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2627, 0.0428],\n",
      "        [0.2080, 0.1180],\n",
      "        [0.1217, 0.7356]])\n"
     ]
    }
   ],
   "source": [
    "print(W_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70b8041-24e4-4bc4-8585-24c3d38056e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.7118, 0.7876],\n",
      "        [0.4183, 0.9014],\n",
      "        [0.9969, 0.7565]])\n"
     ]
    }
   ],
   "source": [
    "print(W_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04512cad-fe73-442a-a741-589ec69748d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Note that we are setting requires_grad=False to reduce clutter in the outputs for illustration purposes. \n",
    "\n",
    "If we were to use the weight matrices for model training, we would set requires_grad=True to update these matrices during model training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb771fd3-7c76-4594-bf68-bf584483c4f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Next, we compute the query, key, and value vectors as shown earlier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6efd5a3-7a42-4a03-932d-2605e12218a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3427, 0.9429])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18823c-886c-464c-9f22-ca488779e5d4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As we can see based on the output for the query, this results in a 2-dimensional vector. \n",
    "\n",
    "This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31d5c8-608b-4a08-8ed4-61c4e8a4864f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "Even though our temporary goal is to only compute the one context vector z(2),  we still require the key and value vectors for all input elements. \n",
    "\n",
    "This is because they are involved in computing the attention weights with respect to the query q(2)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46851a7-1709-4b99-aefc-7ca3697c1c48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "We can obtain all keys and values via matrix multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "785228e7-b21d-44d2-81b5-24787219e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "queries.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "queries = inputs @ W_query\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "print(\"queries.shape:\", queries.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cca6b1-1740-4176-886d-ef8f4f1af0dd",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "As we can tell from the outputs, we successfully projected the 6 input tokens from a 3D onto a 2D embedding space:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12abee21-ff86-415e-88ab-6004b2b4dd64",
   "metadata": {},
   "source": [
    "## 2.2 Scaling Attention Scores to create Attention Weights and Context Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909b52d-ade7-4409-b433-27c181c1871b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "First, let's compute the attention score ω22\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "286ada9f-937a-4c88-b74b-bc64ab13262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3438)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19621f6-06e4-4ff0-a882-8c96f926362a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Again, we can generalize this computation to all attention scores via matrix multiplication:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aaf99ff-2f4c-4a8c-b872-a1b635a24209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9411, 0.3438, 0.3838, 0.7801, 0.2483, 0.6807])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b158fa8-f165-410f-8f8d-bd17dd647a0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We compute the attention weights by scaling the attention scores and using the softmax function we used earlier. \n",
    "\n",
    "The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension of the keys. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df6f8bdf-8e4c-488b-a1a0-6e415e7fe5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights for the second input: tensor([0.2143, 0.1405, 0.1445, 0.1912, 0.1313, 0.1782])\n",
      "Embedding dimension for the keys: 2\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(\"Attention weights for the second input:\", attn_weights_2)\n",
    "print(\"Embedding dimension for the keys:\", d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06d402-7a69-4d5a-9dba-f2386841d911",
   "metadata": {},
   "source": [
    "### Why divide by the square root of the embedding dimension?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e6431-a7a8-4665-b866-6c921f116f4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Reason 1: For stability in learning</b>\n",
    "\n",
    "The softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives almost all the probability mass, and the rest receive very little.ery sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab7a81a4-c5ca-4e78-a9a2-4ff4b46774e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "Softmax after scaling by 8: tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "# Define the tensor\n",
    "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "\n",
    "# Apply softmax without scaling\n",
    "softmax_result = torch.softmax(tensor, dim=-1)\n",
    "print(\"Softmax without scaling:\", softmax_result)\n",
    "\n",
    "# Multiply the tensor by 8 and then apply softmax\n",
    "scaled_tensor = 8 * tensor\n",
    "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
    "print(\"Softmax after scaling by 8:\", softmax_scaled_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fb926-6878-4997-8ef3-3455a7dbe517",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become too large (like multiplying by 8 in this example), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ae8135-0064-4b3c-a385-4a151fd40fb2",
   "metadata": {},
   "source": [
    "### But, why by the square root?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482902e-015e-4aed-9967-ad673df033a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b>Reason 2: To make the variance of the dot product stable</b>\n",
    "\n",
    "The dot product of  Q and K increases the variance because multiplying two random numbers increases the variance.\n",
    "\n",
    "The increase in variance grows with the dimension. \n",
    "\n",
    "Dividing by sqrt (dimension) keeps the variance close to 1\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48d4b220-4684-43b9-9b10-bc56d2e182fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 5.045255667289148\n",
      "Variance after scaling (dim=5): 1.0090511334578296\n",
      "Variance before scaling (dim=20): 22.512670196518346\n",
      "Variance after scaling (dim=20): 1.125633509825917\n"
     ]
    }
   ],
   "source": [
    "# Function to compute variance before and after scaling\n",
    "def compute_variances(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute the products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "\n",
    "        # Compuute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "\n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "\n",
    "    # Calculate the variance of the dot produucts\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "# For dimension 5:\n",
    "variance_before_scaling_5, variance_after_scaling_5 = compute_variances(dim=5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_scaling_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_scaling_5}\")\n",
    "\n",
    "# For dimension 20:\n",
    "variance_before_scaling_20, variance_after_scaling_20 = compute_variances(dim=20)\n",
    "print(f\"Variance before scaling (dim=20): {variance_before_scaling_20}\")\n",
    "print(f\"Variance after scaling (dim=20): {variance_after_scaling_20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f379e-9276-4a91-a410-59033474cd06",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "We now compute the context vector as a weighted sum over the value vectors. \n",
    "\n",
    "Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. \n",
    "\n",
    "We can use matrix multiplication to obtain the output in one step:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75a37cd3-d95c-4e70-b3c8-e4a3bf9222f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector for the second input: tensor([1.1783, 1.3425])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"Context vector for the second input:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aab0da-3816-4988-a9ab-49f98ec0f802",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "So far, we only computed a single context vector, z(2). \n",
    "\n",
    "In the next section, we will generalize the code to compute all context vectors in the input sequence, z(1)to z (T)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85de9318-6aa4-4ea0-b9ae-735197822578",
   "metadata": {},
   "source": [
    "## Implementing a Compact Self Attention Python Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ec0b2-69ab-4c49-a64a-9e6ddebdae03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "In the previous sections, we have gone through a lot of steps to compute the self-attention outputs. \n",
    "\n",
    "This was mainly done for illustration purposes so we could go through one step at a time. \n",
    "\n",
    "In practice, with the LLM implementation in the next chapter in mind, it is helpful to organize this code into a Python class as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f807d67f-60bd-441a-b7bd-a2f45638e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T #omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / d_out**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada6055-933e-457e-a029-fa57d05264e5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this PyTorch code, SelfAttention_v1 is a class derived from nn.Module, which is a fundamental building block of PyTorch models, which provides necessary functionalities for model layer creation and management.    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebeac9f-1dd0-4994-a3b6-5b9ac726ae3f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The __init__ method initializes trainable weight matrices (W_query, W_key, and W_value) for queries, keys, and values, each transforming the input dimension d_in to an output dimension d_out.\n",
    "\n",
    "During the forward pass, using the forward method, we compute the attention scores (attn_scores) by multiplying queries and keys, normalizing these scores using softmax.\n",
    "\n",
    "Finally, we create a context vector by weighting the values with these normalized attention scores.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6c92321-c8d4-41e2-af64-53dc6a17ac8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2705, 1.4457],\n",
      "        [1.1783, 1.3425],\n",
      "        [1.1593, 1.3236],\n",
      "        [1.1985, 1.3688],\n",
      "        [1.1366, 1.2980],\n",
      "        [1.2373, 1.4083]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
